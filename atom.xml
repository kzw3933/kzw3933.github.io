<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>kzw3933&#39;s 个人博客</title>
  
  
  <link href="https://kzw3933.github.io/atom.xml" rel="self"/>
  
  <link href="https://kzw3933.github.io/"/>
  <updated>2024-02-05T06:34:22.384Z</updated>
  <id>https://kzw3933.github.io/</id>
  
  <author>
    <name>kzw3933</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ViT量化推理方案</title>
    <link href="https://kzw3933.github.io/2024/02/05/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/"/>
    <id>https://kzw3933.github.io/2024/02/05/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/</id>
    <published>2024-02-05T06:16:38.000Z</published>
    <updated>2024-02-05T06:34:22.384Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>为使 ViT 在 FPGA 上进行高速推理，计划对 ViT 采用<code>uint8</code>量化。具体来说：对于线性层(<strong>矩阵运算作为核心算子</strong>)的网络层对其输入进行<code>uint8</code>量化，其输出由于一般进行汇总操作,为防止溢出，采用<code>int32</code>；对于非线性层(softmax 和 gelu) 由于无法直接使用量化后的数据进行推理，采用定点数形式(<strong>具体在FPGA上的小数位宽仍需分析这些层的输入输出分布确定</strong>)，也就是说，在前面层量化得到的整数(<strong>一般是<code>int32</code>,因为前面层的输出需要以<code>int32</code>存储</strong>)需要进行反量化后输入到这些非线性层。</p><p>确定了量化的位宽，接下来，就需要考虑量化的方式了，而为了尽可能保持原有模型的精度，<strong>需要考虑模型推理过程中的数据分布来确定具体的量化方式</strong>。量化的方法在CNN领域已经得到充分的研究，一般来说，采用<code>weight</code>进行<code>per-channel量化</code>,各层的输入采用<code>per-tensor</code>的量化，一般都使用Uniform的量化。但是同样的配置在以Transformer为基础的ViT上效果并不好，一些研究工作发现ViT推理过程中经过 <code>LN</code> 层后的输出具有严重的跨通道差异，因此在进入下一层之前采用<code>per-channel</code>量化可以保持模型较高的精度。然而，对这些<code>激活值</code>采用<code>per-channel量化</code>会给推理过程带来较大的负担。在这里，决定采用 <a href="https://arxiv.org/abs/2212.08254">RepQ-ViT</a> 论文中的量化方案，其核心思想是将 <code>LN</code> 层输出的跨通道差异通过仿射变化转移到下一层的<code>weight</code>中，而<code>weight</code>本身就需要进行<code>per-channel</code>的量化, 而且这些调整是在软件端就可完成的，也不会带来<code>FPGA</code>推理上的负担，这样就可以实现<code>激活值</code>的<code>per-tensor</code>量化。除此之外，论文也提到经过<code>softmax</code>计算得到的<code>attention</code>分数具有幂率分布特点，因此采用<code>log量化</code>比较合适，但是使用传统的 $log2$ 量化 间距过大，因而采用  $log\sqrt{2}$ 量化 比较合适，但是 $log2$量化 在硬件实现上很高效，直接使用位运算即可，因此，论文根据 $log2$ 函数和 $log\sqrt{2}$函数的关系，通过分离出一个系数，使用 $log2$ 完成 $log\sqrt{2}$,以达到硬件的高效实现。</p><h2 id="ViT架构"><a href="#ViT架构" class="headerlink" title="ViT架构"></a>ViT架构</h2><p><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205095525415.png"></p><p>由上图可知，ViT 由 Patch Embedding, Transformer Encoder 和 MLP Head 三部分组成。具体如下：</p><ul><li>Patch Embedding: 一层 Conv2d 并加上位置编码</li><li>Transformer Encoder: 具体结构如上图，由L个 Block 组成，每个Block 则由 一个带跳连结构的 MHA 和 一个带跳连结构的 MLP 组成，同时在 MHA 和 MLP 之前需进行 LN 操作。MHA 和 MLP 都是 由矩阵乘积实现。</li><li>MLP Head: 一层线性层</li></ul><h2 id="ViT量化细节"><a href="#ViT量化细节" class="headerlink" title="ViT量化细节"></a>ViT量化细节</h2><p>具体的量化只涉及以矩阵运算作为核心算子的线性层，以下先分析<code>Linear</code>,<code>Matmul</code>和<code>Conv2d</code>的量化，然后分析量化层与非量化层之间的反量化操作。</p><h3 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h3><p>weight采用<code>per-channel</code>的uniform量化,对于input采用<code>per-tensor</code>的uniform量化,bias和输出一样采用<code>int32</code> 保存, 缩放因子采用 input 和 weight 的缩放因子乘积，量化的偏移为0。具体代码如下(<strong>freeze对weight和bias进行量化，这些可在推理前固化，输入量化的过程和量化推理过程在inference中体现</strong>):</p><p><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205113648932.png"></p><p><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205113629986.png"></p><h3 id="Matmul"><a href="#Matmul" class="headerlink" title="Matmul"></a>Matmul</h3><p>Matmul层出现在Attention计算中的$attn &#x3D; Matmul(K, Q)$ 和 $O &#x3D; Matmul(attn, V)$ ，对于 attn 计算的Matmul ，其输入都采用<code>uint8</code>的uniform量化，而对于 O 计算的Matmul，对attn 输入采用   $log\sqrt{2}$ 量化 ，对V输入采用<code>uint8</code>的uniform量化。(<strong>freeze对weight和bias进行量化，这些可在推理前固化，输入量化的过程和量化推理过程在inference中体现</strong>):</p><p><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205113900838.png"></p><p><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205113917325.png"></p><h3 id="Conv2d"><a href="#Conv2d" class="headerlink" title="Conv2d"></a>Conv2d</h3><p>这里与Linear一致，weight采用<code>per-channel</code>的uniform量化,对于input采用<code>per-tensor</code>的uniform量化,bias和输出一样采用<code>int32</code> 保存, 缩放因子采用 input 和 weight 的缩放因子乘积，量化的偏移为0。(<strong>freeze对weight和bias进行量化，这些可在推理前固化，输入量化的过程和量化推理过程在inference中体现</strong>):</p><p><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205113946125.png"></p><p><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205114011648.png"></p><h3 id="量化推理过程"><a href="#量化推理过程" class="headerlink" title="量化推理过程"></a>量化推理过程</h3><p><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205141305562.png"></p><p>如图所示，在非线性层和残差连接以及最后的输出前进行了<strong>反量化操作</strong>,在<code>QConv2d</code>,<code>QLinear</code>和<code>QMatMul</code>的输入进行量化。量化的位宽和量化方式在前面已经介绍。</p><h2 id="模拟验证"><a href="#模拟验证" class="headerlink" title="模拟验证"></a>模拟验证</h2><p>使用 timm 库中开源的<strong>ViT-tiny</strong>模型进行验证, 具体是使用自定义的<code>QConv2d</code>,<code>QLinear</code>,<code>QMatmul</code>提取原有模型被量化的权重参数，并通过前面的<code>inference</code>实现量化推理,供高层的<code>QAttention</code>,<code>QMLP</code>,<code>QVisionTransformer</code>等调实现ViT模型的量化推理(<strong>其中也涉及前面介绍量化推理过程中的在非线性算子前的反量化</strong>)。使用<strong>ImageNet-1k</strong>数据集进行验证，由于train部分数据集过大，下载较慢，直接使用验证集(val_images.tar.gz 6.7G)中的数据进行验证。效果如下:</p><p><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205131532524.png"></p><p>经模拟，量化后的精度损失较小。</p><h2 id="后续改进"><a href="#后续改进" class="headerlink" title="后续改进"></a>后续改进</h2><p>计划尝试Softmax的量化推理方式以及GeLU函数的近似操作。 </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;为使 ViT 在 FPGA 上进行高速推理，计划对 ViT 采用&lt;code&gt;uint8&lt;/code&gt;量化。具体来说：对于线性层(&lt;stron</summary>
      
    
    
    
    <category term="学习总结" scheme="https://kzw3933.github.io/categories/%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="毕业设计" scheme="https://kzw3933.github.io/tags/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="加速器" scheme="https://kzw3933.github.io/tags/%E5%8A%A0%E9%80%9F%E5%99%A8/"/>
    
    <category term="ViT" scheme="https://kzw3933.github.io/tags/ViT/"/>
    
    <category term="量化" scheme="https://kzw3933.github.io/tags/%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>使用C与Rust构建面向终端的编辑器的体验</title>
    <link href="https://kzw3933.github.io/2024/01/30/%E4%BD%BF%E7%94%A8C%E4%B8%8ERust%E6%9E%84%E5%BB%BA%E9%9D%A2%E5%90%91%E7%BB%88%E7%AB%AF%E7%9A%84%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84%E4%BD%93%E9%AA%8C/"/>
    <id>https://kzw3933.github.io/2024/01/30/%E4%BD%BF%E7%94%A8C%E4%B8%8ERust%E6%9E%84%E5%BB%BA%E9%9D%A2%E5%90%91%E7%BB%88%E7%AB%AF%E7%9A%84%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84%E4%BD%93%E9%AA%8C/</id>
    <published>2024-01-30T13:41:11.000Z</published>
    <updated>2024-01-30T14:57:21.089Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>学习分别使用C和Rust构建面向终端的编辑器主要出于以下原因:</p><ul><li>个人对编辑器各种必要的功能和特性的实现比较感兴趣</li><li>刚学习Rust语言，想找一个不大不小的项目练练手，同时想使用C与Rust同时开发加深对Rust相关特性的理解</li><li><strong>project-based-learning</strong>中恰好有使用C构建面向终端的编辑器<strong>kilo</strong>且也有对<strong>kilo</strong>使用Rust构建的开源项目便于学习</li></ul><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><p>所有的计算机软硬件都是对输入完成一定的计算再把输出返回给用户，对于文本编辑器而言,它的<strong>输入</strong>用户使用键盘输入的一系列按键，<strong>计算</strong>则是针对这些不同按键，或命令或普通字符输入按照对应的规则修改当前的<strong>文档</strong>，最后输出则是把<strong>文档</strong>以<strong>合适的格式</strong>展示给用户。</p><p>对于实现面向终端的编辑器，由于操作系统封装了字符设备的使用接口，用户可以直接使用这些接口获得用户的按键输入。然而由于程序在终端中进行，有些按键组合会被终端视为信号或特殊功能处理，且程序的输出也会被终端进行一定的修改，因此，需要进入<strong>raw mode</strong>,禁止那些处理。接着需要对用户输入的普通字符扩充到<strong>文档</strong>中,同时借助一些特殊按键扩充程序的功能(比如编辑器程序的退出，鼠标移动，文档保存,搜索等功能的实现),最后将文档内容显示在终端上供用户审阅,而输出的实现具体来说就是将原始的普通文本字符以及其中穿插的颜色控制字符，清屏，清行，控制鼠标位置的控制字符或<strong>escape sequence</strong>组成的字符串序列发送到标准输出。</p><h2 id="C实现模块划分"><a href="#C实现模块划分" class="headerlink" title="C实现模块划分"></a>C实现模块划分</h2><ul><li>editor(实现整个编辑器状态记录和更改，如进入raw mode,文档插入,删除)</li><li>highlight(记录具体的文本类型的高亮格式，如关键字，注释，数字和字符串的显示)</li><li>row(具体承担对文档中一行的内容的处理，如插入字符,高亮控制字符的插入等)</li><li>terminal(承担终端信息如分辨率的获取，用户按键输入的获取，鼠标的移动和滚屏等)</li></ul><h2 id="Rust实现模块划分"><a href="#Rust实现模块划分" class="headerlink" title="Rust实现模块划分"></a>Rust实现模块划分</h2><ul><li>editor(实现整个编辑器状态的记录和更改)</li><li>document(承担对文档内容的修改)</li><li>row(承担对文档中行的内容的修改)</li><li>terminal(承担终端的输入，鼠标的移动和清屏等操作)</li><li>filetype(控制具体文档类型的高亮格式)</li><li>highlighting(处理具体的高亮类型的渲染)</li></ul><p>从模块划分上，两者的实现基本一致，但是使用Rust实现明显感觉语义上更加清晰，使用更加安全(主要是降低了内存安全问题出现的可能),一方面是由于Rust中的<strong>Enum</strong>可以为多类型的值给予一种语义上的类型，另外则是Rust无需用户手动管理内存，而是基于所有权和生命期的机制实现内存的释放(编译期即可决定)。另外，就是作为一门现代的编程语言，使用Rust构建项目更加便捷，项目的组织，模块的划分和具体功能的承担和隔离对管理项目的复杂度比C的体验要好。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;学习分别使用C和Rust构建面向终端的编辑器主要出于以下原因:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;个人对编辑器各种必要的功能和特性的实现比较感兴趣&lt;</summary>
      
    
    
    
    <category term="学习总结" scheme="https://kzw3933.github.io/categories/%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="编程语言" scheme="https://kzw3933.github.io/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    <category term="C" scheme="https://kzw3933.github.io/tags/C/"/>
    
    <category term="Rust" scheme="https://kzw3933.github.io/tags/Rust/"/>
    
    <category term="编辑器" scheme="https://kzw3933.github.io/tags/%E7%BC%96%E8%BE%91%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu(双系统)安装、美化以及日常使用</title>
    <link href="https://kzw3933.github.io/2024/01/07/Ubuntu(%E5%8F%8C%E7%B3%BB%E7%BB%9F)%E5%AE%89%E8%A3%85%E3%80%81%E7%BE%8E%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%97%A5%E5%B8%B8%E4%BD%BF%E7%94%A8/"/>
    <id>https://kzw3933.github.io/2024/01/07/Ubuntu(%E5%8F%8C%E7%B3%BB%E7%BB%9F)%E5%AE%89%E8%A3%85%E3%80%81%E7%BE%8E%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%97%A5%E5%B8%B8%E4%BD%BF%E7%94%A8/</id>
    <published>2024-01-07T01:55:55.000Z</published>
    <updated>2024-01-07T02:05:40.829Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于喜欢折腾和经常删除系统进重装，且有时会因此重装前忘记备份自己之前的一些配置，遂在此进行记录，方便之后的安装。</p><h2 id="Ubuntu安装"><a href="#Ubuntu安装" class="headerlink" title="Ubuntu安装"></a>Ubuntu安装</h2><h2 id="Ubuntu美化"><a href="#Ubuntu美化" class="headerlink" title="Ubuntu美化"></a>Ubuntu美化</h2><p>个人不喜欢太复杂的美化，在此只进行了gnome主题,图标主题,一些gnome extension的安装，以及zsh的配置。</p><h2 id="日常使用"><a href="#日常使用" class="headerlink" title="日常使用"></a>日常使用</h2><p>这里记录了自己经常使用的一些软件。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;由于喜欢折腾和经常删除系统进重装，且有时会因此重装前忘记备份自己之前的一些配置，遂在此进行记录，方便之后的安装。&lt;/p&gt;
&lt;h2 id=&quot;U</summary>
      
    
    
    
    <category term="笔记教程" scheme="https://kzw3933.github.io/categories/%E7%AC%94%E8%AE%B0%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="Ubuntu安装" scheme="https://kzw3933.github.io/tags/Ubuntu%E5%AE%89%E8%A3%85/"/>
    
    <category term="美化" scheme="https://kzw3933.github.io/tags/%E7%BE%8E%E5%8C%96/"/>
    
    <category term="日常使用" scheme="https://kzw3933.github.io/tags/%E6%97%A5%E5%B8%B8%E4%BD%BF%E7%94%A8/"/>
    
  </entry>
  
  <entry>
    <title>编程语言学习总结</title>
    <link href="https://kzw3933.github.io/2024/01/04/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    <id>https://kzw3933.github.io/2024/01/04/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</id>
    <published>2024-01-04T04:57:53.000Z</published>
    <updated>2024-01-07T02:13:37.060Z</updated>
    
    <content type="html"><![CDATA[<p><strong>计算机科学中的所有问题都可以通过增加一个间接层来解决。         – David Wheeler</strong></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本科期间,从计算机程序设计,计算机组成原理,操作系统,再到编译原理,算法,同时自学了多种编程语言,对编程语言的诞生,新编程语言的提出,不同编程语言之间的异同或者说是改进有了一些自己的体会。</p><p><strong>个人理解,计算机编程的目的在于借助计算机的硬件功能来帮助我们自行解决可计算的问题。但是不同的问题，显然，它们的解决方法或者说是计算过程(这里讨论可计算问题)是不同的。一个自然的想法是看我们自己是如何解决不同的问题的，把这个计算过程抽离出来也就形成了针对对应问题的算法。那么，如果假设，我们已经有了这样一个支持计算的计算机，也有了待解决问题的计算过程(算法),接下来的问题就是如何把这个计算过程映射到计算机上。而编程，或者说是编程语言，最初就是从计算机的角度描述这个算法的,但是，随着，抽象层次的提高和从高层次到低层次自动转换工具(编译器等工具)的出现，我们可以以接近自然语言的语法描述这个计算过程。这可以视为增加了一个抽象层隔离了底层的繁琐细节,控制了复杂度。</strong></p><p><strong>总结来说，编程语言的就是为了把算法映射到具体硬件上的计算过程描述，与此同时，随着问题规模与复杂度的提升，编程语言的层次更高，提供了抽象细节，控制复杂度的功能。</strong></p><h2 id="图灵的基本思想"><a href="#图灵的基本思想" class="headerlink" title="图灵的基本思想"></a>图灵的基本思想</h2><p>图灵机,又称确定型图灵机，是英国数学家艾伦·图灵于1936年提出的一种将人的计算行为抽象化的数学逻辑机，其更抽象的意义为一种计算模型，可以看作等价于任何有限逻辑数学过程的终极强大逻辑机器。</p><p>图灵的基本思想是用机器来模拟人们用纸笔进行数学运算的过程，他把这样的过程看作下列两种简单的动作：</p><ul><li>在纸上写上或擦除某个符号</li><li>把注意力从纸的一处移动到另一处<br>而在每个阶段，人要决定下一步的动作，依赖于（a）此人当前所关注的纸上某个位置的符号和（b）此人当前思维的状态</li></ul><p>为了模拟人的这种运算过程，图灵构造出一台假想的机器，该机器由以下几个部分组成：</p><ol><li>一条无限长的纸带<strong>TAPE</strong>。纸带被划分为一个接一个的小格子，每个格子上包含一个来自有限字母表的符号，字母表中有一个特殊的符号$\square$表示空白。纸带上的格子从左到右依次被编号为0, 1, 2, …，纸带的右端可以无限伸展。</li><li>一个读写头<strong>HEAD</strong>。它可以在纸带上左右移动，能读出当前所指的格子上的符号，并能改变它。</li><li>一套控制规则数量有限的<strong>TABLE</strong>（A finite table of instructions）。它根据当前机器所处的状态以及当前读写头所指的格子上的符号来确定读写头下一步的动作，并改变状态寄存器的值，令机器进入一个新的状态。<br>按照以下顺序告知图灵机命令：<ul><li>写入（替换）或擦除当前符号；</li><li>移动 <strong>HEAD</strong>， ‘L’向左， ‘R’向右或者’N’不移动；</li><li>保持当前状态或者转到另一状态。</li></ul></li><li>一个<strong>状态寄存器</strong>。它用来保存图灵机当前所处的状态。图灵机的所有可能状态的数目是有限的，并且有一个特殊的状态，称为停机状态。</li></ol><p>注意这个机器的每一部分都是有限的，但它有一个潜在的无限长的纸带，因此这种机器只是一个理想的设备。图灵认为这样的一台机器就能模拟人类所能进行的任何计算过程。</p><h2 id="冯诺伊曼体系结构"><a href="#冯诺伊曼体系结构" class="headerlink" title="冯诺伊曼体系结构"></a>冯诺伊曼体系结构</h2><p>冯诺伊曼结构是一种将程序指令存储器和数据存储器合并在一起的电脑设计概念结构。它描述的是一种实现通用图灵机的计算设备，以及一种相对于并行计算的序列式结构参考模型。本结构隐约指导了将存储设备与中央处理器分开的概念，因此依本结构设计出的计算机又称存储程序计算机。</p><p>存储程序计算机在体系结构上主要特点有：</p><ul><li>以运算单元为中心</li><li>采用存储程序原理</li><li>存储器是按地址访问、线性编址的空间</li><li>控制流由指令流产生</li><li>指令由操作码和地址码组成</li><li>数据以二进制编码</li></ul><p>数学家冯·诺依曼提出了计算机制造的三个基本原则（采用二进制、程序存储、顺序执行），以及计算机的五个组成部分（运算器、控制器、存储器、输入设备、输出设备），这套理论被称为冯·诺依曼体系结构，根据这一原理制造的计算机被称为冯·诺依曼结构计算机。</p><p><strong>图灵机是理论模型，是对人计算过程的模拟；而冯诺依曼计算机则是图灵机的工程化实现，程序就是图灵机中的纸带。图灵机需要这么一根无限长的纸带，但是在现实中，不存在无限的内存,因此冯诺依曼模型实现出来的更类似有限状态自动机。诺依曼体系相对之前的计算机最大的创新在于程序和数据的存储，以此实现机器内部编程。</strong></p><h2 id="编程语言的发展"><a href="#编程语言的发展" class="headerlink" title="编程语言的发展"></a>编程语言的发展</h2><ul><li>机器语言和汇编语言<br>早期计算机使用机器语言和汇编语言进行编程，对硬件具有直接的映射。</li><li>Fortran（1957年）<br>第一个高级编程语言，用于科学和工程计算。</li><li>LISP（1958年）<br>用于人工智能研究的编程语言，具有强大的符号处理能力。</li><li>COBOL（1959年）<br>面向业务的编程语言，用于商业和金融应用。</li><li>C语言（1972年）<br>由贝尔实验室的丹尼斯·里奇（Dennis Ritchie）和肯·汤普逊（Ken Thompson）开发，成为后来许多编程语言的基础。</li><li>面向对象编程（OOP）<br>1980年代，引入了面向对象的编程语言，如Smalltalk、C++等。</li><li>脚本语言和动态语言<br>1990年代，出现了脚本语言（例如Python、Perl）和动态语言（例如Ruby）</li></ul><p><strong>计算机的出现就是为了高效，快速，自动化解决人类遇到的问题。而为了可以解决不同的问题，进而有了存储程序(可编程)的概念,因而，也就有了程序编写(面向计算机的计算过程描述)的问题，随着编程项目工程复杂度和成本的提高，工具本身的改进也就成了必然,因此，编程语言的不断发展改进也就很容易理解了。</strong></p><h2 id="编程语言的组成与功能"><a href="#编程语言的组成与功能" class="headerlink" title="编程语言的组成与功能"></a>编程语言的组成与功能</h2><p>编程语言的从最早的机器语言，以面向机器的方式描述计算过程到现在的高级编程语言以人类友好的方式描述计算过程。曾在一本编程书中看到这样的总结，<strong>计算机科学家思考问题的方式兼具数学、工程和自然科学的优点:计算机科学家像数学家那样使用规范的语言来描绘概念，具体来说就是计算；像工程师那样设计，将各个部分组装成系统并权衡不同的解决方案；像科学家那样观察复杂系统的行为，进而作出假设并进行验证</strong>。同时，计算机科学归根到底是借助计算机去自动解决人们需要解决的问题,自然是具有工程性的。工程上的问题在解决时需要重点考虑到的就是控制其复杂度。&lt;&lt;计算机程序的构造与解释&gt;&gt;中总结了编程项目中控制复杂度的方法：<code>黑盒抽象</code>、<code>约定接口</code>、<code>创建一种新的语言</code>,另外也总结了程序设计中的最基本原则:<code>原语</code>、<code>组合</code>、<code>抽象</code>，这也是编程语言所提供的功能。因此，过程式编程,面向对象编程,函数式编程,以及各种语言提供的各种或方便或统一的语言要素也就不难理解了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;计算机科学中的所有问题都可以通过增加一个间接层来解决。         – David Wheeler&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/</summary>
      
    
    
    
    <category term="学习总结" scheme="https://kzw3933.github.io/categories/%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="编程语言" scheme="https://kzw3933.github.io/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
  </entry>
  
  <entry>
    <title>Transformer加速器学习总结</title>
    <link href="https://kzw3933.github.io/2024/01/03/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    <id>https://kzw3933.github.io/2024/01/03/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</id>
    <published>2024-01-03T13:45:48.000Z</published>
    <updated>2024-01-05T08:25:09.177Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本科毕业课题为<code>基于FPGA的通用Transformer加速器设计与优化</code>,对相关知识和工具进行了学习，并阅读了一些相关的论文，目前总结如下。</p><h2 id="课程学习"><a href="#课程学习" class="headerlink" title="课程学习"></a>课程学习</h2><h3 id="EE290-2伯克利机器学习硬件课程"><a href="#EE290-2伯克利机器学习硬件课程" class="headerlink" title="EE290-2伯克利机器学习硬件课程"></a><a href="https://inst.eecs.berkeley.edu/~ee290-2/sp21/">EE290-2</a>伯克利<code>机器学习硬件</code>课程</h3><ul><li><p>Quantization<br>介绍了模型量化的基本知识。</p></li><li><p>Core computation in DNN<br>分析了DNN中的各个组件,包括<code>卷积</code>,<code>全连接</code>,<code>池化</code>,<code>批量归一化</code>,<code>激活</code>,核心算子为<code>卷积</code>和<code>矩阵乘法</code>,同时介绍了<code>卷积</code>的几种实现。</p></li><li><p>Execution order of the core computation<br>介绍了硬件加速的核心原则<code>Locality</code>和<code>Parallelism</code>,并分析了卷积具体实现时的执行顺序(dataflow)。</p></li><li><p>Hardware realization of the core computation<br>介绍了<code>CPU Inefficiency</code>的原因,以及DNN核心算子硬件优化的几个方面:<code>Inst. decoding logic</code>,<code>Datapath</code>和<code>Memory system</code>,并介绍了一个具体的例子<code>Google TPU</code>。</p></li><li><p>Mapping DNNs to hardware<br>介绍了由于硬件资源的约束, 需要将算法从软件上的抽象描述进行分块,循环等操作映射到具体的硬件电路执行。这其中涉及到不同的超参数可能导致不同的加速效果,因此会有large mapping space的搜索问题。</p></li><li><p>Data Orchestration<br>介绍了跨存储层次的数据传输机制，介绍了<code>直接与间接</code>和<code>耦合与非耦合</code>两个维度的区别。</p></li><li><p>Sparsity<br>介绍了DNN中的存储和计算中的稀疏性。</p></li><li><p>Codesign<br>介绍了软硬件协同设计的概念和案例。</p></li><li><p>Other Operators and Near-Data Processing<br>介绍除了<code>卷积</code>和<code>矩阵乘法</code>外的一些算子并介绍了一些专有硬件中的Near-Data Processing。</p></li><li><p>Accelerator-Level Parallelism<br>从BLP,ILP,TLP,DLP到ALP,介绍了各个层次上的并行并介绍了移动芯片上加速器级别的集成以及此级别上的并行性以及它的挑战和机遇。</p></li></ul><p>总的来说，<code>ee290-2</code>课程从AI算法核心算子提取，核心算子硬件实现,从数据流，存储局部性,并行性进行优化，算法部署,软硬件协同设计等方面系统介绍了面向AI算法进行加速器设计的流程。</p><h2 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h2><h3 id="推荐会议-1"><a href="#推荐会议-1" class="headerlink" title="推荐会议[1]"></a>推荐会议<sup id="fnref:1"><a href="#fn:1">[1]</a></sup></h3><h4 id="计算机体系结构顶级会议"><a href="#计算机体系结构顶级会议" class="headerlink" title="计算机体系结构顶级会议"></a>计算机体系结构顶级会议</h4><ul><li><p>ISCA(国际计算机体系结构研讨会) <code>CCF-A</code><br><code>ISCA</code>是计算机体系结构领域的顶级会议。其关注计算机体系结构和系统的设计与分析。ISCA 上的论文以其深度和完整性而闻名，提供了对计算机体系结构前沿研究的全面了解。</p></li><li><p>HPCA(高性能计算机体系结构研讨会) <code>CCF-A</code><br><code>HPCA</code>是计算机体系结构领域的另一顶级会议。其强调高性能计算和体系结构创新。HPCA 上的论文通常探讨实现计算系统高性能的新颖思想和方法。</p></li><li><p>MICRO(国际微体系结构研讨会) <code>CCF-A</code><br><code>MICRO</code>关注计算机系统的微体系结构层面。其深入研究处理器内各个组件的组织和设计细节。尽管更为详细，但它对系统性能的整体理解有着重要贡献。</p></li><li><p>ASPLOS(用于编程语言和操作系统的体系结构支持) <code>CCF-A</code><br><code>ASPLOS</code>是一个涵盖计算机体系结构、编程语言和操作系统交叉领域的会议。它经常展示这些不同方面之间的协同作用。</p></li></ul><p><code>计算机体系结构四大顶会</code>,上面的论文系统性、完整性很强，新手读起来吃力，但此类论文处于领域前沿，读完有全局概念  </p><h4 id="EDA顶级会议"><a href="#EDA顶级会议" class="headerlink" title="EDA顶级会议"></a>EDA顶级会议</h4><ul><li><p>DAC(设计自动化会议) <code>CCF-A</code><br><code>DAC</code>是专注于电子系统设计和自动化的会议，包括硬件和软件设计。它以实用和面向工程的研究而闻名，是电子设计自动化领域的研究人员和从业者的宝贵平台。</p></li><li><p>ICCAD(国际计算机辅助设计会议) <code>CCF-B</code><br><code>ICCAD</code>专注于电子系统的计算机辅助设计技术。它为电子集成电路和系统设计中使用的工具和方法的研究提供了一个平台。</p></li></ul><p>想法都不错，工程性相比于<code>计算机体系结构四大顶会</code>弱很多</p><h4 id="FPGA顶级会议"><a href="#FPGA顶级会议" class="headerlink" title="FPGA顶级会议"></a>FPGA顶级会议</h4><ul><li><p>FPGA <code>CCF-B</code><br><code>FPGA</code>是FPGA领域最重要的顶级会议，旨在展现与FPGA技术相关所有领域的最新进展，如基本逻辑电路和架构、计算机辅助设计、高层次综合、工具和模型、处理器和系统、测试方法、应用开发等</p></li><li><p>FCCM <code>CCF-C</code><br><code>FCCM</code>专注于定制计算机体系结构和可编程逻辑器件的会议。研究方向包括 FPGA 的创新应用、自定义硬件加速器的设计等。</p></li><li><p>FPL <code>CCF-C</code><br><code>FPL</code>是一个关注 FPGA 和可编程逻辑领域的国际会议。它覆盖了从 FPGA 架构到应用的广泛主题。</p></li><li><p>FPT <code>CCF-C</code><br><code>FPT</code>是一个关注可编程技术的国际会议，包括 FPGA 和其他可编程设备的研究和应用。</p></li></ul><p>这四个会议创新型依次下降，但是工程性都比较强</p><h3 id="论文主题"><a href="#论文主题" class="headerlink" title="论文主题"></a>论文主题</h3><p>主要阅读了<code>Transformer架构及其变种</code>,<code>模型压缩</code>和<code>加速器设计</code>方面的论文,相关可见<a href="../%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E9%98%B6%E6%AE%B5%E6%80%A7%E6%80%BB%E7%BB%93-%E4%B8%80/index.html">论文阅读阶段性总结(一)</a></p><h2 id="工具学习"><a href="#工具学习" class="headerlink" title="工具学习"></a>工具学习</h2><h3 id="HLS学习资源汇总"><a href="#HLS学习资源汇总" class="headerlink" title="HLS学习资源汇总"></a>HLS学习资源汇总</h3><p>HLS（High-Level Synthesis）是一种用于硬件设计的自动化工具，它允许工程师使用高级编程语言来描述硬件功能，而无需深入了解硬件描述语言（HDL）如 Verilog 或 VHDL。有关HLS相关资料和教程主要参考Xilinx的官方资料教程</p><ul><li><p>Vivado Design Suite Tutorial: High-Level Synthesis (UG871 v2021.1 2021.3.26)<br>一组较小的教程，解释并演示了使用高层次综合将C、C++、SystemC代码转换成RTL实现过程中的所有步骤。循序渐进的介绍了<code>C验证</code>,<code>接口综合</code>,<code>优化设计</code>,<code>RTL验证</code>,<code>在IP整合器中使用HLS IP</code>，<code>在Zynq SoC设计中使用HLS IP</code>和<code>在DSP系统生成器中使用HLS IP</code>等内容。值得一提的是，教程关于优化的部分详细分析了如何使用Vitis HLS分析一个现有设计存在的问题，以及如何使用<code>HLS pragma</code>逐步优化以达到要求的吞吐率。教程中的项目源码可以<a href="https://github.com/jmduarte/HLS_hls4ml_Tutorial">HLS_hls4ml_Tutorial</a>找到,原教程使用的HLS工具版本较早,最新的HLS工具会自动完成一些优化，因此会出现和教程里不一致的情况,可以尝试较早版本的HLS工具学习。</p></li><li><p>High-Level Synthesis (UG902 v2019.2 2020.1.13)<br>高层次综合的用户指南,介绍了<code>高层次综合</code>,<code>高层次综合C语言库</code>,<code>高层次综合编码样式</code>,<code>高层次综合参考指南</code>主题的各个方面,可用作用户手册。</p></li><li><p>Introduction to FPGA Design with Vivado High-Level Synthesis(UG998 v1.1 2019.1.22)<br>面向采用Vivado高层次综合开展FPGA设计的简介,对FPGA,硬件设计的概念,Vivado高层次综合等方面进行了介绍,值得一提的是资料中前两章从编程模型角度分析了传统FPGA开发和处理器上的开发的不同，介绍HLS的出现消除了这种编程模型的差异,也从并行性上对比了FPGA和处理器,可以把HLS看作面向FPGA的C++并行编程的范式(由于FPGA提供给HLS编译器一个可变的后端,因此它可对算法的并行性进行更多的探索,用户也可以通过<code>pragma</code>的方式进行一定的约束或者指定目标)，从此视角，我们可以更好的理解HLS编译器的工作以及使用HLS进行硬件设计的概念。</p></li><li><p>Vitis High-Level Synthesis User Guide (UG1399 v2023.1 2023.7.17)<br>Vitis高层次综合用户指南,和<code>UG902</code>类似,涵盖了<code>HLS编程指南</code>,<code>使用Vitis HLS</code>,<code>Vitis HLS命令参考资料</code>,<code>Vitis HLS C语言驱动程序参考资料</code>,<code>Vitis HLS参考资料</code>和<code>Vitis HLS移植指南</code>主题的各个方面，可用作用户手册。</p></li></ul><h3 id="Chisel学习资源汇总"><a href="#Chisel学习资源汇总" class="headerlink" title="Chisel学习资源汇总"></a>Chisel学习资源汇总</h3><ul><li><p>Digital Design With Chisel<br>一本硬件设计的导论，专注于使用硬件构建语言Chisel,展示了小到中型的硬件部分，用于探索Chisel进行硬件设计。涵盖了使用Chisel进行数字电路设计的基础知识和例子,但是需要有一定的scala和chisel的基础,书中并没有涵盖对相关语法和项目搭建的完整或系统性解释。网上可以找到对应的中文版，但是翻译存在一定的问题。</p></li><li><p><a href="https://github.com/freechipsproject/chisel-bootcamp">chisel-bootcamp</a><br>github上的使用chisel设计数字电路的实战教程,项目以<code>jupyter notebook</code>形式组织，提供了搭建运行<code>jupyter notebook</code>教程环境的项目说明,依次介绍了<code>scala介绍</code>,<code>chisel设计数字电路基础</code>,<code>chisel设计数字电路进阶</code>,<code>firrtl</code>相关的知识,较为系统并且方便实操。</p></li></ul><p><a href="#fnref:1" id="fn:1">[1]: 评级参考&lt;&lt;中国计算机学会推荐国际学术会议和期刊目录-2022&gt;&gt;</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本科毕业课题为&lt;code&gt;基于FPGA的通用Transformer加速器设计与优化&lt;/code&gt;,对相关知识和工具进行了学习，并阅读了一些相</summary>
      
    
    
    
    <category term="学习总结" scheme="https://kzw3933.github.io/categories/%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="加速器设计" scheme="https://kzw3933.github.io/tags/%E5%8A%A0%E9%80%9F%E5%99%A8%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="transformer" scheme="https://kzw3933.github.io/tags/transformer/"/>
    
    <category term="资源汇总" scheme="https://kzw3933.github.io/tags/%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读阶段性总结(一)</title>
    <link href="https://kzw3933.github.io/2024/01/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E9%98%B6%E6%AE%B5%E6%80%A7%E6%80%BB%E7%BB%93-%E4%B8%80/"/>
    <id>https://kzw3933.github.io/2024/01/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E9%98%B6%E6%AE%B5%E6%80%A7%E6%80%BB%E7%BB%93-%E4%B8%80/</id>
    <published>2024-01-03T12:07:46.000Z</published>
    <updated>2024-01-30T13:44:41.624Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本科毕业设计的题目是<code>基于FPGA的通用Transformer加速器设计与优化</code>,于大四上学期调研学习关于<code>Transformer模型架构</code>，<code>模型压缩</code>，<code>硬件加速器</code>相关方面的论文。由于之前没有进行过较多论文的阅读，也就没有对论文作阅读笔记的习惯，但是随着论文阅读的增多，发现自己对之前阅读的论文的核心改进和思想常常遗忘。借着寒假开始的机会，在此，做一个论文阅读及论文核心思想的汇总。</p><h2 id="论文分类汇总"><a href="#论文分类汇总" class="headerlink" title="论文分类汇总"></a>论文分类汇总</h2><h3 id="Transformer及其变种"><a href="#Transformer及其变种" class="headerlink" title="Transformer及其变种"></a>Transformer及其变种</h3><p><strong>1.Attention Is All You Need 【NIPS 2017】</strong></p><p>论文作者提出了一种完全基于注意力机制，摒弃了递归和卷积的新的神经网络架构<code>Transformer</code>,在<code>WMT 2014 English-to-French</code>翻译任务上取得了最好的<code>BLEU</code>分数,并且展示了Transformer对于其他任务也具有很好的泛化性。</p><p>模型架构如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-04%2020-25-14.png"></p><p>注意力计算方式如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-04%2020-25-32.png"></p><p>由模型架构图可以看出,<code>Transformer</code>仍然是属于<code>Encoder-Decoder</code>架构, 由以下几部分组成: <code>Input Embedding</code>, <code>Output Embedding</code>, <code>Positional Encoding</code>,<code>Encoder Block</code>,<code>Decoder Block</code>。</p><p>模型各个组件的功能如下:</p><ul><li><p><code>Embedding</code>: 无论是输入嵌入层还是输出嵌入层都是完成将词语嵌入为词向量，这个过程类似于查表(将某个词语编号映射为它对应的词向量)。</p></li><li><p><code>Encoder Block</code>: 每个<code>Encoder Block</code>都由带有残差连接并作LN的多头注意力和FFN组成。如上面展示的注意力计算方式图，首先单头自注意力(<code>MHA</code>)实际上使用三个权重矩阵$W_q$,$W_k$,$W_v$对输入进行矩阵乘法完成$Q$,$K$,$V$的特征的提取。再将$Q$与$K$运算(论文中使用<code>scaled dot product</code>后作softmax)获得每个$Q$分量对每个$V$分量的分数,进而根据这个分数完成每个$Q$分量对整个$V$的信息提取。而多头自注意力可以简单看成是将单头自注意力的$Q$,$K$,$V$特征维度进行同样的划分,最后将划分后提取到的V的信息再按照特征维度进行拼接,不过最后还要再与一个$W_p$进行矩阵乘积，只有这样，才能具有融合这些被划分到不同head里的特征的能力。<strong>由于Transformer整个运算过程中序列长度的维度没有变化,因此可以追踪针对序列中某个token对其他token注意力的关注</strong>。另外的FFN则是两层全连接,首先将<code>MHA</code>的输出的特征维度从$dim$映射到$4dim$,然后再映射到$dim$(当然，这也是残差连接所要求的)。当然，在这两个模块的输出后还需要进行残差连接和LN。</p></li><li><p><code>Decoder Block</code>: 与<code>Encoder Block</code>类似，每层<code>Decoder Block</code>都由带有残差连接并作LN的掩码多头注意力机制,多头注意力机制，FFN组成。多头注意力的计算方式和<code>Encoder Block</code>一致。但是不同的是,对于Decoder来说,比如翻译任务，前面翻译的词不应该看到后面翻译的词，因此，需要对$Q$与$K$计算得到的score，进行相应位置的抹除(也就是置0)。而第二个多头注意力也与<code>Encoder Block</code>不同，它的$Q$,确实还是从前一层或者输入经过一个权重矩阵提取特征而来，但是$K$和$V$则是对<code>Encoder</code>最后的输出经过权重矩阵提取特征得到。而<code>FFN</code>层则一致。当然，在这三个模块的输出后还需要进行残差连接和LN。<strong>需要注意的是整个<code>Decoder</code>模块的输出是自回归的，需要逐个输出。(这一点倒是和RNN类的模型一致了)</strong></p></li><li><p><code>Positional Encoding</code>,由于<code>Transformer</code>对输入使用自注意力提取信息，而这种方式是没有位置差异的，因此，为了包含NLP领域的语句的位置信息，必然需要某种位置信息的嵌入，论文中采用了基于三角函数的位置信息的嵌入，这里不做展开。</p></li></ul><p><strong>2.BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 【NAACL 2018】</strong></p><p><code>BERT</code>模型采用了多层的<code>Transformer</code>编码器堆叠在一起，其中<code>BERT-base</code>对应的是12层encoder,<code>BERT-large</code>对应的是24层的encoder。另外由于<code>BERT</code>是想成为语言模型的预训练模型以支持NLP领域的迁移学习，它的训练任务就需要是NLP领域的通用任务，以能提取输入的良好特征表示便于微调用于其他任务。</p><p>具体来说,<code>BERT</code>使用两个任务来训练:</p><ul><li>将输入文本序列的部分（15%）单词随机Mask掉，让<code>BERT</code>来预测这些被Mask的词语</li><li>判断两个句子是否是相邻句子。</li></ul><p>模型架构如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-04%2020-46-48.png"></p><p><strong>3.An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale【ICLR 2021】</strong></p><p>由于<code>Transformer</code>模型在NLP领域的成功，促使论文作者将<code>Transformer</code>模型应用于图像。作者将图片划分为patches(具体来说，是把图片划分成一个一个的小块，论文里是16*16,这样图片就转换成这样一个一个小块的序列了,类似于NLP领域语句的token),剩下就是把这些patches组成的序列，输入标准的<code>Transformer</code>中(当然由于任务的不同，这里实际上只用到了<code>Transformer</code>的Encoder模块)。由于论文中作者作了图片分类的任务,在除了图片本身划分的patches外，额外添加一个patch(具体实现是作为可学习参数),在模型的最后提取这个patch，经过一个全连接层进行分类。</p><p>模型架构如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-04%2020-27-33.png"></p><p><strong>4.Training data-efficient image transformers &amp; distillation through attention 【ICML 2021】</strong></p><p>由于<code>ViT</code>模型的训练需要大量的训练数据，论文作者采用知识蒸馏的方式训练<code>ViT</code>模型解决这个问题。具体的知识蒸馏方式是，相对于<code>ViT</code>增加了一个<code>distillation token</code>，其对应的token输出值与teacher model的输出值尽可能接近。</p><p>蒸馏过程如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-04%2020-48-44.png"></p><p><strong>5.Swin Transformer: Hierarchical Vision Transformer using Shifted Windows 【ICCV 2021】</strong><br>将<code>Transformer</code>从语言应用到视觉的挑战主要来自两个方面，与文本的单词相比，视觉领域的视觉实体的规模变化很大以及图像中像素的分辨率变化。为了解决这个问题，作者提出了一种分层的<code>Transformer</code>并使用了<code>shifted windows</code>方案。<code>shifted windows</code>方案通过将自注意力计算限制在不重叠的局部窗口上，同时允许跨窗口连接，从而带来了更高的效率。这种分层架构可以灵活地在不同的尺度上建模，并且具有相对于图像大小的线性计算复杂度。<code>Swin Transformer</code>的这些特性使其与广泛的视觉任务兼容,展示了基于<code>transformer</code>的模型作为视觉骨干的潜力。</p><ol><li><p><strong>整体架构</strong><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-17%2010-22-46.png"><br>对经过嵌入的<code>patch tokens</code>应用了几个具有修改自注意力计算（Swin Transformer 块）的 Transformer 块。Transformer 块维护<code>token</code>的数量 ($\frac{H}{4} × \frac{W}{4}$ )，以及线性嵌入称为<strong>阶段 1</strong>。为了生成分层表示，随着网络的深入，<code>patch merging</code>层减少了<code>token</code>的数量。第一个<code>patch merging</code>层连接每组 2 × 2 个相邻<code>patch</code>的特征，并在 <code>4C-dimensional concatenated</code>特征上应用线性层。这将<code>token</code>的数量减少了 2 × 2 &#x3D; 4 的倍数（分辨率的 2 倍下采样），输出维度设置为 2C。之后应用 <code>Swin Transformer</code> 块进行特征变换，分辨率保持在 $\frac{H}{8} × \frac{W}{8}$。补丁合并和特征变换的第一个块表示为<strong>阶段 2</strong>。该过程重复两次，分别为<strong>阶段 3</strong>和<strong>阶段 4</strong>，输出分辨率为 $\frac{H}{16} × \frac{W}{16}$ 和 $\frac{H}{32} × \frac{W}{32}$。这些层共同产生一个分层表示。<br>其中的<code>Swin Transformer Block</code>是通过将 Transformer 块中的标准多头自注意力 (MSA) 模块替换为基于<code>shifted windows</code>的模块来构建的，其他层保持不变。</p></li><li><p><strong>基于shifted window的自注意力</strong><br>标准的 <code>Transformer</code> 架构及其对图像分类的变种都进行了全局自注意力，其中计算<code>token</code>与所有其他<code>token</code>之间的关系。全局计算导致了<code>token</code>数量的二次复杂度，这使得它不适合许多需要大量<code>token</code>进行密集预测或表示高分辨率图像的视觉问题。因此，作者提出在<code>local window</code>里进行自注意力计算。</p></li></ol><ul><li>Self-attention in non-overlapped windows<br>为了有效地建模，作者提出在局部窗口内计算自注意力。窗口被安排以不重叠的方式均匀地划分图像。假设每个窗口包含 $M × M$ patch，全局 MSA 模块和基于窗口的模块的计算复杂度在$h × w$ patch的图像上为<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-17%2010-41-57.png"></li><li>Shifted window partitioning in successive blocks<br>基于窗口的自我注意模块缺乏跨窗口的连接，这限制了其建模能力。为了在保持非重叠窗口的有效计算的同时引入跨窗口连接，论文提出了一种移位窗口分区方法，该方法在连续 <code>Swin Transformer</code> 块中的两种分区配置之间交替。<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-17%2010-44-12.png"><br>相应的计算流程如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-17%2010-47-05.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-17%2010-47-21.png"></li><li>Efficient batch computation for shifted configuration<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-17%2010-49-44.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-17%2010-49-55.png"></li></ul><p><strong>由于加速器具体设计，量化，剪枝往往针对具体的模型架构并使用相应的模型进行实验，因此就需要对<code>Transformer</code>及其变种的模型架构方面的论文进行了解。<code>CV</code>领域主要有<code>ViT</code>,<code>DeiT</code>,<code>SwinTransformer</code>(这个架构目前还没有看过)。<code>NLP</code>领域主要有<code>Transformer</code>,<code>Bert</code>,<code>GPT</code>(目前还未阅读过)。</strong></p><h3 id="模型压缩"><a href="#模型压缩" class="headerlink" title="模型压缩"></a>模型压缩</h3><h4 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h4><p><strong>1.SELECTIVE BRAIN DAMAGE: MEASURING THE DIS-PARATE IMPACT OF MODEL PRUNING 【ICLR 2020】</strong></p><p>深度学习模型运行需要大量的计算、内存和功耗，为了解决模型模型运行的瓶颈，研究者提出了一系列模型压缩方法，其中就包括模型剪枝，能够有效地减小内存、功耗，提高计算效率。然而，“每一枚硬币都有正反两面”，模型剪枝在获得诸多益处的同时，势必也会造成一定的“舍”。这些损失到底是什么？针对不同的模型以及在不同的场景下，模型剪枝产生的影响又有何不同呢？在这篇论文中，研究人员提出了一个正式的框架，该框架用于识别在剪枝和未剪枝模型之间的有巨大分歧或泛化能力差异的类别和图像。发现引入稀疏性对剪枝已识别的示例（Pruning Identified Exemplars ，<code>PIE</code>）和类别的系统影响更大。其中 <code>PIE</code> 是在一组独立训练的剪枝模型和未剪枝模型之间最频繁产生不同的预测结果的图像。对于剪枝模型和未剪枝模型而言，对 PIE 图像进行分类都更加困难。这篇论文的工作被总结如下:</p><ul><li>剪枝最好被描述为“选择性脑损伤”。剪枝对每个类别的影响都不一样；稀疏性的引入对一小部分类别会产生不成比的系统影响。</li><li>称受剪枝影响最大的示例为“ 剪枝已识别的示例”（PIE），剪枝和未剪枝模型对它进行分类都更加困难。</li><li>剪枝会大大降低图像损坏和自然对立图像的稳健性。</li></ul><p><strong>2.DepGraph: Towards Any Structural Pruning 【CVPR2023】</strong></p><p>这篇论文主要介绍了两方面的内容: 第一部分介绍Torch-Pruning工具，一种通用的结构化剪枝库，并通过实例演示如何快速实现结构化剪枝;第二部分侧重Torch-Pruning的底层算法DepGraph，主要讨论如何建模结构化剪枝中的层依赖，实现任意结构的剪枝。具体来说，由于结构化剪枝(如剪去CNN中的某个通道)会影响模型结构，而模型的相邻层的结构是有依赖的，这就需要对前后层的结构进行调整，这个过程同样是递归的，而论文中的这个工具会根据输入的待剪枝通道自动分析待剪枝模型中的依赖，自动完成前后相关的通道的移除。</p><p><strong>看的剪枝方面的论文似乎都集中在<code>CNN</code>领域,面向<code>Transformer</code>的剪枝方面的论文似乎比较少。</strong></p><h4 id="量化"><a href="#量化" class="headerlink" title="量化"></a>量化</h4><p><strong>1.Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference 【CVPR 2018】</strong></p><p>针对智能移动设备的日益普及和基于深度学习的模型巨大的计算成本要求的高效准确的设备上推理方案的情况，论文作者提出了量化和训练神经网络模型以达到完全使用整数在设备上进行推理的方法。主要作出以下贡献: (1) 提出一个<strong>量化方案（quantization scheme）</strong>,把权重和激活值都量化到8位整数，少数的bias量化到32位整数 (2) 提出一个<strong>纯整数量化推理框架（quantized inference framework）</strong>，以及在ARM NEON上的高效实现 (3) 提出一个<strong>量化训练框架（quantized training framework）</strong>，最小化量化损失。</p><p>量化推理和训练过程如下：<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2009-47-02.png"></p><ol><li><p><strong>量化方案</strong><br>这个量化方案同时应用在量化推理(整数)和量化训练(浮点)中。数学定义如下: $r$表示实数，$q$表示其量化后的值,两者的关系通过以下仿射变换定义:$$r&#x3D;S(q-Z)$$其中$S (scale)$是一个浮点数,$Z (zero point)$是整数。具体实现把一个浮点数映射到具有固定位宽的整数上，还需要进行取整和截断操作。需要注意的是,这里的$S$在量化推理中实际上采用了以下方式$M&#x3D;2^{-n}M_0$,其中$M0$是整数，进行近似计算，从而避免了浮点运算。</p></li><li><p><strong>纯整数量化推理框架</strong><br>把原浮点数据进行量化后还需要考虑如何使用这些量化后的数据进行运算,如果只能再反量化回去运算，量化也就没有了意义。论文以矩阵乘法为例，介绍了如何对其进行量化推理。具体过程如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2010-00-03.png"><br>这里的M是浮点数，具体实现也可采用以上提到的方案，借助$M&#x3D;2^{-n}M_0$,其中$M0$是整数，进行近似计算，避免浮点运算。当然，为了计算上的优化，有些计算是可以提前完成的:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2010-03-54.png"><br>需要注意的是,在具体的量化推理中对于进行运算的两个矩阵使用<code>uint8</code>,而对结果和偏置(bias)使用<code>uint32</code>,因此对于<code>bias</code>的量化参数简单采用$S_{bias}&#x3D;S_1S_2$和$Z_{bias} &#x3D; 0$。<br>其他层的量化推理中，将BN层合并到前一层的卷积中(BN层的操作可以合并到卷积的权重和偏置中),从而不需考虑;tanh、softmax等函数，其定点实现和浮点实现差不多，因此不必使用查询表;一些网络有Addition层，把两个input简单相加。处理量化值的时候略微麻烦一些，因为两个input的scale参数不同，要把其中一个input 通过定点乘数$M&#x3D;S_1&#x2F;S_2$ rescale到另一个input的scale。然后Addition对这些整数简单相加即可。至于concat层也和Addition一样面临rescale问题。但concat不使用rescale这种有损运算，而是规定concat的所有input和所有output都保持相同的量化参数，避免这个问题。</p></li><li><p><strong>量化训练框架</strong><br>如前面展示的量化推理和训练过程图中,量化训练主要通过插入模拟量化节点(具体来说是在量化后再反量化回浮点数值)在模型训练中引入量化的影响。但是反向传播不变(具体来说因为插入的量化节点的运算并不可导，因此在反向传播时是直接跳过，将梯度向前传)。</p></li></ol><p><strong>2.Data-Free Quantization Through Weight Equalization and Bias Correction 【ICCV 2019】</strong></p><p>8-bit的量化对于深度学习硬件上高效的推理是必要的，但是把一个模型量化到8-bit却并不是一个简单的任务，往往导致要么巨大的模型性能下降要么需要花费大量的时间训练适应这样量化的模型。论文作者提出了一种对于深度神经网络<code>data-free</code>的量化方法。它在计算机视觉的架构和任务上实现了接近原始模型的性能。这种量化方法依赖于两个策略: 均衡网络中的权重范围和偏置矫正。</p><ol><li><p><strong>Cross-layer range equalization</strong><br>作者提到他们的动机是当把<code>fp32</code>的模型量化到<code>int8</code>时伴随着很大的模型性能下降，而这种下降可以通过采用<code>per-channel</code>量化,微调或者两者来达到原始模型的性能。而<code>per-channel</code>方式能提升模型性能是因为神经网络权重参数不同通道的数据分布差异很大，如果采用原始的<code>per-tensor</code>量化，对整个tensor使用相同的量化参数，会造成原始信息的大量损失。论文中作者采用在层之间均衡不同通道的权重范围，从而缓解<code>per-tensor</code>的问题。具体过程如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2012-20-15.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2012-20-40.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2012-23-46.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2012-21-16.png"></p></li><li><p><strong>Quantization bias correction</strong><br>之前的研究中一个常见的假设是量化带来的损失是无偏的，因此在层的输出中抵消，确保层的输出的平均值不会随着量化的结果而改变。但是论文作者的研究显示权重上的量化误差可能会在相应的输出上引入有偏差的误差。这改变了下一层的输入分布，这可能会导致不可预测的影响。因此，有必要进行偏差的矫正。矫正方法如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2012-30-01.png"><br>值得注意的是，论文也提到了使用了BN和没使用BN时偏差矫正值的计算,详细见论文。</p></li></ol><p><strong>3.Up or Down? Adaptive Rounding for Post-Training Quantization 【ICML 2020】</strong><br>在对神经网络进行量化时，把浮点权重量化到最接近的定点数是主流的方法，但是，这并不是最优的。在这篇论文中，作者提出AdaRound,一种面向PTQ的适应数据和任务损失的<code>weight-rounding</code>方法。作者对预训练网络量化中的<code>rounding</code>问题进行了理论分析，并使用<code>泰勒展开</code>对任务损失函数进行近似，从而把这个问题转化为一个<code>QUBO</code>问题，最后对这个问题进行简化，转换为一个<code>layer-wise</code>的局部损失,并使用一种<code>soft relaxation</code>的方法优化这个损失。论文结果表明，其效果超过主流的<code>rounding-to-nearest</code>的方法。</p><p>相关记号如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-16%2023-22-29.png"></p><ol><li><p><strong>Task loss based rounding</strong><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-16%2023-29-10.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-16%2023-29-43.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-16%2023-29-59.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-16%2023-30-10.png"><br>以上过程从任务损失的角度形式化了量化过程中量化的weight的变化对整个模型性能的影响，并通过<code>泰勒展开</code>的近似以及忽略层与层之间的关系，把一个全模型优化问题简化成一个逐层优化问题。</p></li><li><p><strong>From Taylor expansion to local loss</strong><br>由于Hessian矩阵计算的困难，论文中对问题进行了简化。<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-17%2000-13-00.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-17%2000-13-10.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-16%2023-45-56.png"></p></li><li><p><strong>AdaRound</strong><br>经过以上的简化后的问题是一个在离散空间中的优化问题，无法直接通过数学优化的方式求出最优解，因此，作者提出<code>AdaRound</code>的方法，把离散空间松弛成连续空间再进行优化。<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-16%2023-58-10.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-16%2023-58-24.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-16%2023-58-38.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-16%2023-58-47.png"></p></li></ol><p>需要注意的是，<code>AdaRound</code> 虽然也是后量化训练的算法，但它仍然是需要做优化训练的，只不过相对于量化训练来说，它不需要有完整的训练数据集，只需要用一小部分数据 (且不需要标签) <code>finetune</code>就可以。</p><p><strong>4.Fully Quantized Transformer for Machine Translation 【EMNLP 2020】</strong><br>论文提出了面向机器翻译的<code>Transformer</code>的完全量化的方法。这篇论文介绍了他们对<code>Transformer</code>中各层的输入、输出，参数是否量化以及怎样量化进行了介绍。</p><ol><li><p><strong>首先，他们的量化方案是使用均匀量化</strong><br>量化方案如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2014-12-50.png"></p></li><li><p><strong>其次，作者介绍了他们对那些操作进行了量化</strong><br>他们选择量化所有可以在推理时提供计算速度增益的操作。对所有矩阵乘法进行量化，这意味着 <code>MatMul</code> 的输入和权重都将都是$k$位量化的。量化的其他操作包括除法，但只有当分子和分母都是二阶或高阶张量时。对于所有其他操作，例如总和，量化这些操作添加的计算成本超过了以降低精度执行操作的好处。因此，不量化这样的操作。<br>更准确地说，他们量化了 <code>Transformer</code>的所有权重，但是不包括偏差。后者与矩阵乘法的INT32输出相加。此外，与权重矩阵相比，偏差的内存空间微不足道，占总权重的 0.1% 以上。对于位置嵌入，这些是固定的，因此可以在训练之前量化一次。LN 的$\gamma$权重也被量化。至于激活，我们使用编码器和解码器中的位置编码来量化输入嵌入的总和。在多头注意力中，我们量化 (Q, K, V ) 输入、softmax 的分子、softmax 的分母、softmax 的输出和 Scaled Dot-Product Attention 的输出。在推理时，softmax 不需要以全精度计算。事实上，指数函数可以用阶跃函数代替。对于位置前馈网络，我们量化了 ReLU 的输出和前馈本身。最后，对于所有LayerNorms，我们量化分子$x−\mu$、分母$\sqrt{\sigma^2+\epsilon}$、它们的商和LayerNorm的输出。<br>他们使用<code>bucketing</code>的方法来量化每个张量的的子集，而不是对每个量化张量使用一组 $(s, x_{min})$ 。尽管这增加了更多的标量，但内存成本总体上是微不足道的。此外，添加的灵活性可以极大地缓解所有值映射到单个低数值精度域所产生的精度损失。我们对所有权重矩阵使用这种分桶方法，许多子集等于输出维度。对于激活，我们在量化时使用分桶：输入嵌入与位置编码、Q、K、V 输入、Scaled Dot-Product Attention 的输出、前馈的输出、LayerNorm 的分子、商和输出的总和。</p></li></ol><p><strong>5.Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks 【ICLR 2020】</strong></p><p>深度神经网络 (DNN) 在各种实际应用中取得了显着的改进。然而，巨大的内存和计算成本阻碍了 DNN 的质量部署，例如在资源受限的设备上。为了减少内存占用和计算负担，人们广泛探索了<code>量化</code>、<code>修剪</code>和<code>低秩分解</code>等几种模型压缩方法。在本文中，作者专注于神经网络量化以实现有效的推理。量化过程中涉及两种操作，即<code>裁剪</code>和<code>投影</code>。如果裁剪操作超出范围，则将全精度数字设置为范围边界；投影操作将每个数字（裁剪后）映射到预定义的量化级别（固定数字）。可以看到这两种操作都会造成信息丢失。一个好的量化方法应该解决以下两个问题&#x2F;挑战，分别对应于两个矛盾。<br><strong>如何确定最佳裁剪阈值以平衡裁剪范围和投影分辨率?</strong> 分辨率表示两个量化级别之间的间隔；间隔越小，分辨率越高。第一个矛盾是，给定固定数量的比特来表示权重，范围和分辨率成反比。例如，更大的范围可以剪辑更少的权重；然而，分辨率变得更低，从而损害投影。需要注意的是，异常值的贸然裁剪可能会极大地危及网络，尽管它们可能只需要一层中所有权重的 1-2%。以前的工作尝试了预定义的 或可训练的裁剪阈值，但如何在训练期间自动找到最佳阈值仍未解决。<br><strong>如何在设计量化层次时兼顾计算效率和权重分布?</strong> 现有的大多数量化方法使用均匀量化，尽管非均匀量化通常可以获得更好的准确度。原因是针对均匀量化水平的<code>投影</code>对硬件更友好。<strong>然而，实证研究表明，DNN层中的权重遵循钟形长尾分布，而不是均匀分布。</strong>换句话说，相当比例的权重集中在平均值(峰值面积)周围;还有一些权重相对较大，超出了量化范围(称为离群值)。这种分布也存在于激活中。第二个矛盾是:考虑到权值的钟形分布，在均值附近分配更高的分辨率(即较小的量化间隔)是很有动机的;然而，这种不均匀的量化水平会带来很高的计算开销。<code>Powers-of-Two</code>量化然后被提出，因为它通过硬件上的移位操作实现了廉价的乘法，并且在平均值附近具有超高分辨率。然而，当比特宽度增加时，普通的<code>PoT</code>方法只增加了平均值附近的分辨率，而完全忽略了其他区域。<br>针对以上问题，论文作者提出了<code>Additive Powers-of-Two</code>量化。</p><ol><li><p><strong>Additive Powers-of-Two 量化(APoT)</strong><br>量化相关的符号约定如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2015-02-00.png"><br><code>Powers-of-Two</code>量化可表示如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2014-50-18.png"><br><code>Additive Powers-of-Two</code>量化可表示如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2014-49-50.png"><br>使用均匀量化、<code>PoT</code>量化、<code>APoT</code>量化的加速器图示如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2014-50-41.png"></p></li><li><p><strong>Reparameterized Clipping Function(RCF)</strong><br>作者提到在量化中要权衡考虑<code>裁剪范围</code>和<code>投影分辨率</code>,因此阈值$\alpha$也是一个很重要的要考虑的量。考虑到权值的分布可能是复杂的，并且在不同的层和训练步骤之间是不同的，因此所有层的静态裁剪阈值$\alpha$不是最优的。为此，作者提出措施如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2015-13-39.png"></p></li><li><p><strong>Weight Normalization(WN)</strong><br>关于Weight Normalization的过程如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2015-16-50.png"></p></li></ol><p>最后，对于一个使用了APoT的卷积层来说，论文展示其前向传播和反向传播过程如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2015-18-42.png"></p><p><strong>6.Understanding and Overcoming the Challenges of Efficient Transformer Quantization 【EMNLP 2021】</strong></p><p>基于 Transformer 的架构已成为各种自然语言处理任务的事实标准模型。然而，对于资源有限的设备的高效部署和推理，它们的内存占用和低延迟是令人望而却步的。在这项工作中，作者探索了<code>Transformer</code>的量化。论文表明，<code>Transformer</code>具有独特的量化挑战——即难以用低位定点格式表示的高度动态的激活值范围。这些激活值包含残差连接中的结构性的异常值，以鼓励特定的注意力模式，例如关注特殊的分隔符标记。具体来说，作者在对<code>BERT</code>模型进行标准的<code>8-bit</code>的量化时发现了不同任务上模型性能的下降(不同任务似乎对量化的容忍性是不同的因此性能下降幅度也不相同)，通过消融实验发现对前馈神经网络后的残差和的量化导致最大的性能下降。他们还注意到结构性的异常值与特殊的 [SEP] 标记的相关性。为了解决以上问题，作者提出基于<code>训练后量化</code>和<code>量化感知的训练</code>提出三种解决方法并对比了效果。</p><ol><li><p><strong>Mixed precision PTQ</strong><br>对导致问题的激活张量采用<code>16-bit</code>的量化，例如前馈神经网络后的残差和。更高的位宽将提供一个具有足够精度的模型来表示 FFN 的输入和输出，以及它们的总和。另外，在实验中发现<code>BERT</code>模型似乎对 8 位权重量化具有相当大的弹性，因此，他们还考虑了低位 (2-4) 权重和token嵌入量化的影响，这将模型大小减少了 8× 以上，精度损失较小。</p></li><li><p><strong>Per-embedding-group PTQ</strong><br>提高量化模型的性能的一种方法是增加量化粒度。基于实验的观察，激活张量中最有问题的异常值在少数指定的嵌入维度中，因此考虑对单个嵌入维度或嵌入维度组具有不同的量化参数。我们首先描述每个嵌入激活量化。在类似 BERT 的模型中，中间隐藏激活张量 x 的形状为 (B, T, d)，其中 B 是批量大小，T 是序列长度，d 是嵌入维度的数量（<code>BERT-base</code> 的 d &#x3D; 768）。受每通道权重量化的启发，可以在每个嵌入维度都有不同的缩放因子和零点，而不是对整个张量有两个标量。<br>原始量化、每嵌入维度量化、每嵌入组量化后的matrix-vector乘积公式分别如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2016-07-37.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2016-07-48.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2016-08-09.png"><br>考虑到计算量，可采用<code>Per-embedding-group</code>的量化，需要注意的是这种量化需要对原始tensor进行置换排列操作以按照以上图示中的公式计算，因此在计算完成后还需要进行这种置换排列的逆操作，详细可见论文。</p></li><li><p><strong>Quantization-aware training</strong><br>通过在模型训练过程引入模拟量化引入量化的影响，来训练适应量化的模型。</p></li></ol><p><strong>7.BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction 【ICLR 2021】</strong></p><p><strong>8.Post-Training Quantization for Vision Transformer 【NIPS 2021】</strong></p><p><code>Transformer</code>在各种计算机视觉应用中取得了显著的性能。与主流的卷积神经网络相比,<code>Vision Transformer</code>通常是提取强大特征表示的复杂架构，但是在移动设备上更难开发。在论文中，作者提出了一种有效的训练后量化算法来减少视觉转换器的内存存储和计算成本。基本上，量化任务可以分别看作是寻找权值和输入的最优低位量化区间。为了保持注意力机制的功能，他们在传统的量化目标中引入了排名损失，旨在保持量化后自注意力结果的相对顺序。此外，还深入分析了不同层的量化损失和特征多样性之间的关系，并通过利用每个注意图和输出特征的核范数来探索混合精度量化方案。</p><ol><li><p><strong>Ranking-Aware Post-Training Quantization</strong><br>自注意力层是 <code>Transformer</code> 的关键组件，因为它可以计算特征的全局相关性，这使得 <code>Transformer</code> 不同于卷积神经网络。对于自注意力的计算，作者发现，量化后注意力图的相对顺序发生了变化，这可能会导致显着的性能下降。因此，他们提出了引入排名损失:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2017-49-37.png"><br>这是一个优化问题，作者提出一种简单有效的迭代搜索方法来优化<code>Transformer</code>中这些均匀量化的参数。首先输入的量化区间$\Delta_l^X$固定，优化$\Delta_l^W$,然后固定后者优化前者，直到目标函数收敛或达到最大迭代次数。<br>另外，为了进一步减少因量化带来的偏置损失，作者也进行了偏置矫正。</p></li><li><p><strong>Nuclear Norm Based Mixed-Precision Quantization</strong><br>不同的<code>Transformer</code>层关注不同的结构，预计它们表现出不同的灵敏度。因此，为所有层分配相同数量的位宽是次优的。因此，作者探索了混合精度量化，其中更多的比特被分配给更敏感的层，以保持性能。考虑到<code>Transformer</code>层的独特结构，对MSA或MLP模块中的所有操作分配相同的位宽。这也将对硬件实现友好，因为权重和输入被分配相同的位宽。而对这些位宽的选择，作者采用以下方式:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2018-02-57.png"></p></li></ol><p><strong>9.FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer 【IJCAI 2022】</strong></p><p>网络量化显著降低了模型推理的复杂性，并被广泛应用于实际部署中。然而，大多数现有的量化方法主要是在卷积神经网络(<code>CNN</code>)上开发的，当应用于完全量化的<code>Vision Transformer</code>时，会遭受严重的性能下降。在这项工作中，作者发现许多这些困难是由于LN的输入存在严重的通道间差异，并提出了<code>Power-of-Two Factor(PTF)</code>，这是一种降低完全量化的<code>Vision Transformer</code>的性能下降和推理复杂性的系统方法。此外，通过观察注意力图中的极端非均匀分布，作者还提出了 Log-Int-Softmax (LIS)，通过使用 4 位量化和 BitShift 算子来维持原有分布特征并简化推理。</p><p>关于量化的相关记号表达如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2018-36-48.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2018-37-11.png"></p><ol><li><p><strong>Power-of-Two Factor for LayerNorm Quantization(PTF)</strong><br>作者观察到，在应用量化到LN层中并训练后观察到显着的性能下降,并发现原因是LN的输入具有较大的通道间差异。基于这种极端的通道间变化，将相同的量化参数应用于所有通道的逐层量化将导致不可容忍的量化误差。一种可能的解决方案是使用分组量化或通道量化，它将不同的量化参数分配给不同的组或通道。然而，这些仍然会导致浮点域中的均值和方差的计算，从而导致较高的硬件开销。在这篇论文中，作者提出了<code>Power-of-Two Factor</code>的量化方法:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2018-42-25.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2018-42-52.png"></p></li><li><p><strong>Log-Int-Softmax for Softmax Quantization(LIS)</strong><br>多头自注意力（MSA）是基于<code>Transformer</code>的架构中最重要的组件之一，但由于其对于token数量(图片分辨率除以patch大小)的二次复杂度，它被认为是最资源密集型的计算。由于模型性能被证明受益于更高的分辨率和更小的patch大小 ，当提高分辨率和减小patch大小时，注意力图的存储和计算成为直接影响推理吞吐量和延迟的瓶颈。因此，较小的注意力图和更有效的推理成为一个迫切的需求。为了压缩<code>attention map</code>,作者将其量化到更低的位宽。在分析注意图的分布时观察到以相当小的值为中心的分布，而只有少数异常值的值接近 1,因此采用$log2$量化更为合适。相关量化过程如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2018-59-46.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2018-58-44.png"><br>以前的工作往往选择不量化 Softmax，因为 Softmax 中的计算量可以忽略不计，量化可能会导致显着的精度下降。然而，在CPU和GPU&#x2F;NPU之间移动的数据，进行去量化和重新量化，会给硬件设计带来很大的困难，这不是可以忽略的消耗。因此，作者提出了结合上面的$log2$量化和$i-exp$(exp函数的一种多项式近似)提出<code>Log-Int-Softmax</code>用于softmax的量化推理。<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2018-53-35.png"><br>没采用<code>LIS</code>和采用<code>LIS</code>的MSA的推理过程如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2018-56-03.png"><br>正常 MSA 和使用了<code>LIS</code>的方法之间的差异如图所示(每个阶段标记了数据类型)。在左侧显示未量化 Softmax 的多头自注意力中，查询 (Q) 和键 (K) 的矩阵乘法需要在 Softmax 之前被去量化到全精度，并在之后重新量化。当采用Log-IntSoftmax时，如右图所示，整个数据类型可以是纯整数，量化尺度单独并行计算。值得注意的是，LIS 在注意力图上使用积极的 4 位表示，这显着减少了内存占用。</p></li></ol><p><strong>10.PTQ4ViT: Post-Training Quantization Framework for Vision Transformers with Twin Uniform Quantization 【ECCV 2022】</strong></p><p><strong>11.RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers 【ICCV 2023】</strong></p><p>模型量化是压缩网络的有效方法，为了实现较高的量化性能，许多研究实施QAT的方法，但是这种方法是资源密集型且耗时的，因此，PTQ作为一种低成本和快速部署的解决方案被人们广泛研究。如DFQ,AdaRound,BRECQ等PTQ方法在CNN上取得较好效果，但由于网络结构的诸多不同，这些方法在ViT上性能较差,因此，为ViT设计PTQ的方案得到广泛关注。但是这些为ViT设计的量化方案，往往为了考虑硬件上的效率采用面向硬件的简单量化器，不能很好的表示极端分布，进而导致量化后的较大性能下降。本文作者采用<strong>量化和推理过程解耦</strong>，前者采用复杂的量化器，后者采用尺度重新参数化的简单量化器的方案，这样确保了<strong>准确的量化和高效的推理</strong>。更具体的说，作者集中在两个具有极端分布的组件: post-LayerNorm activations 和 post-Softmax activations，前者具有严重的通道间差异，后者的分布具有幂律特征。首先对这两个组件分别进行channel-wise量化和$log\sqrt{2}$量化，然后再进行尺度重新参数化转化为硬件友好的layer-wise量化和$log2$量化进行推理。</p><ol><li><p><strong>Scale Reparam for LayerNorm Activations</strong><br><strong>在ViT中的LN操作如下</strong>:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-14%2013-33-36.png"><br>对于经过LN后的激活值，由于其具有严重的通道间差异，因此首先进行<code>channel-wise</code>的量化，接着将其转化为<code>layer-wise</code>的量化,对于转换后的<code>layer-wise</code>的量化，一个自然的想法是设置其量化参数为：<code>scale</code>为各通道<code>scale</code>的均值,<code>zero-point</code>为各通道<code>zero-point</code>的均值,而<strong>再参数化处理</strong>，就是进行代数式的等价变形,具体过程如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-14%2013-44-14.png"><br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-14%2013-44-40.png"></p></li><li><p><strong>Scale Reparam for Softmax Activations</strong><br>关于将$log\sqrt{2}$的量化转化为$log2$的量化，本质也是进行一系列的等价变形，具体过程如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-14%2014-03-36.png"></p></li></ol><p><strong>从以上不同的量化过程可以看出，量化是压缩模型，减小计算代价的有效措施，而量化能够有这方面的效果是因为模型训练过程需要使用<code>fp32</code>来存储更多的信息以捕捉梯度的变化进行参数的学习训练，但是在推理过程，往往只需要较低的位宽就可以完成任务。在对模型进行量化时，从量化的公式就可以看出，量化过程伴随着<code>截断损失</code>和<code>投影损失</code>,这里涉及阈值，位宽以及量化方式的选择。当然，在整个网络的量化中也伴随着量化粒度的选择。这些选择涉及到数据本身的分布，不同粒度下数据是集中还是分散，因为本质上低位宽能够表达的信息自然比高位宽要少，如果在某粒度下的数据分散程度高，从信息的角度来说就是蕴含的信息多，自然无法使用低位宽表示。而不同的量化论文间的不同之处也体现在它们面对的任务的不同，训练出的模型参数及激活值的数据分布情况，以及为解决这些情况或更改量化方式，改变量化粒度甚至调整数据分布等。</strong></p><h3 id="软硬件协同加速器设计"><a href="#软硬件协同加速器设计" class="headerlink" title="软硬件协同加速器设计"></a>软硬件协同加速器设计</h3><p><strong>1.GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference 【MICRO 2020】</strong></p><p>基于注意力的模型在各种自然语言理解任务中取得了显著的成功，然而高效的执行这些模型由于它们大量的参数面临着存储的限制。论文作者提出了<code>GOBO</code>,一种模型量化方法，能够压缩性能最好的<code>BERT</code>模型及其变种的绝大多数<code>fp32</code>参数到<code>3-bit</code>同时维持它们的准确度。不同于其他的量化方法，<code>GOBO</code>不需要微调或者再训练来补偿量化带来的误差。论文中作者提到了<code>GOBO</code>的两个实践性的应用。首先，它减少了内存存储和流量，从而降低了推理延迟和能耗。它的这种内存压缩机制与许多体系结构兼容。其次，作者展示了一个协同设计的硬件单元。总的来说,<code>GOBO</code>保持了绝大部分的权重为<code>3-bit</code>,即使是在计算过程中，其具有以下特性: (i)使处理单元(PE)面积高效，允许我们在单位面积上打包更多的计算能力，(ii)用加法替换大多数乘法累加，(iii)通过放大片上内存容量来减少片外流量</p><p>这篇论文中的量化和普通的量化不同(也许可以归类到广义的量化中去，可以认为它把待量化的tensor映射到一张表的索引,而那张表则存储着<strong>量化</strong>后的值，说是量化后的值，实际上是对原tensor中的数值进行聚类后得到的值),同时基于这种间接的量化设计了硬件上实现的方案。</p><ol><li><p><strong>GOBO QUANTIZATION</strong><br>对于 <code>BERT</code>，<code>GOBO</code> 在层的粒度和微调模型上运行。压缩过程首先将权重分为两组，“Gaussian”（G）和“Outliers”（O）。<code>GOBO</code> 将异常值存储为 (<code>fp32</code>)，而它将“G”组值量化为几个代表性值。只有一小部分权重，通常小于 0.1%，最终出现在“O”组中。<code>GOBO</code>通过量化”G”组来减少整体模型大小。由于权重分布不均匀，作者提出了一种非线性量化方法，该方法可以在密集填充权重的情况下获得更高的分辨率。这种方法证明能够以 8 个具有代表性的 fp32 值对大约 99.9% 的权重进行编码，而推理误差保持在 1% 以下（或 16 个代表性值，没有精度损失）。“G”组权重存储为这些代表性值的 <code>3-bit</code> 索引。<code>GOBO</code> 每层仅使用一组具有代表性的值。总之，<code>GOBO</code> 存储每层以下信息：（1）原始形式的异常值。(<code>fp32</code>) (2) 每个“G”组权重的 bin 索引。(<code>3-bit</code>) (3) 表示每个 bin 的代表性值（质心）的权重的重建表。(<code>fp32</code>)<br>论文中使用高斯分布外加一个阈值辨别离群值。而对”G”组数据的代表性值使用类似<code>K-Means</code>的方法求解。<br><code>QOBO</code>算法如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-05%2020-45-53.png"></p></li><li><p><strong>COMPUTE ACCELERATION</strong><br>大多数计算发生在 FC 层，每个层都是一个权重矩阵和激活向量乘法。使用非常小的字典允许从许多“查找将权重索引转换为其 fp32 质心，然后使用 fp32 激活的乘法累加”（每个权重一个）转换为每个权重索引的 fp32 激活累积，然后是几个 fp32 乘法累加操作（每个输出激活每个质心一个）。也就是说，我们不是首先将每个权重解码为其质心，然后与相应的激活相乘，而是在给定权重索引的情况下，将相应的激活累积到每个质心总和中，然后对所有量化的权重这样做后，可以将这几个总和与质心相乘。</p></li></ol><p>论文中也给出了<code>GOBO Hardware Accelerator</code>的设计，细节较多，简单来说就是支持使用<code>GOBO</code>这种间接量化的表示进行<code>矩阵向量乘法计算加速</code>的硬件实现,具体可参考论文。</p><p><strong>2.A$^3$: Accelerating Attention Mechanisms in Neural Networks with Approximation 【HPCA 2020】</strong></p><p><strong>3.Hardware Accelerator for Multi-Head Attention and Position-Wise Feed-Forward in the Transformer 【SOCC 2020】</strong><br>大多数现有的加速器都是为卷积神经网络 (<code>CNN</code>) 或循环神经网络 (<code>RNN</code>) 构建的。目前，<code>Transformer</code> 模型正在取代自然语言处理 (NLP) 领域的 <code>RNN</code>。然而，由于涉及密集的矩阵计算和复杂的数据流，<code>Transformer</code> 模型的硬件设计从未被报道。在本文中，作者提出了第一个用于两个关键组件的硬件加速器，即多头注意力 <code>(MHA) ResBlock</code> 和<code>位置前馈网络 (FFN) ResBlock</code>，它们是 <code>Transformer</code> 中两个最复杂的层。首先，引入了一种有效的方法来划分 <code>Transformer</code> 中的巨大矩阵，允许两个 ResBlock 共享大部分硬件资源。其次，计算流程设计良好，以确保<code>脉动阵列</code>的硬件利用率高，这是设计中最大的模块。第三，对复杂的非线性函数进行了高度优化，以进一步降低硬件复杂度和整个系统的延迟。作者的设计使用硬件描述语言 (HDL) 进行编码，并在 Xilinx FPGA 上进行评估。与具有相同设置的 GPU 上的实现相比，所提出的设计在 MHA ResBlock 中分别展示了 14.6 倍和 FFN ResBlock 中的 3.4 倍的加速。</p><ol><li><p><strong>PARTITIONING MATRICES IN THE FFN AND THE MHA</strong><br>考虑到 Transformer 架构的特性，论文中所提出的硬件加速器不仅可以加速 MHA ResBlock，还可以加速 FFN ResBlock。为了确保MHA ResBlock和FFN ResBlock可以重用硬件资源，论文作者首先从矩阵运算的角度分析这两个ResBlocks，然后给出一种划分矩阵的方法，使所有一般的矩阵矩阵乘法(GEMMs)都可以用同一个脉动阵列(SA)来完成，其大小限制在$s × 64$，其中<code>s</code>即为最大的序列长度,<code>64</code>则为分头后的特征维度。对MHA和FFN中的tensor分析如下,假设 FFN 的输入称为 $X$，张量 $X$ 的形状与 $Q$ 相同（MHA 的输入张量之一），即 [$\text{batch_size}$, $\text{seq_len_v}$, $d_{model}$]。此外，张量 $K$ 始终与张量 $V$具有相同的维度，其形状为 [$\text{batch_size}$, $\text{seq_len_v}$, $d_{model}$]。在正常情况下，$\text{seq_len_q}$ 等于 $\text{seq_len_v}$，因此所有四个张量的形状可以表示为[$\text{batch_size}$, $s$, $d_{model}$]。假设批大小等于1，这两个ResBlocks的计算可以被认为是矩阵运算的集合。显然，$s×64$的SA可以支持所有MHA中基于多头的线性子层中的所有矩阵乘法。然而，如何在更大的矩阵之间完成其他乘法s是需要考虑的另一个重要问题。这就需要权重矩阵的<code>partition</code>了,具体来说就是要在$d_{model}$维度上作<code>partition</code>。<br>MHA ResBlock和FFN ResBlock的计算展示如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-06%2012-04-09.png"><br>MHA ResBlock和FFN ResBlock的计算流如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-06%2012-08-20.png"></p></li><li><p><strong>HARDWARE ARCHITECTURE DESIGN FOR THEPROPOSED ACCELERATOR</strong><br>论文中也对<code>Transformer</code>模型中<code>MHA ResBlock</code>和<code>FFN ResBlock</code>中的softmax和LN计算的进行了介绍，并提出了整体的加速器架构。由于细节较多，具体可参考论文。<br>整体的加速器架构如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-06%2012-15-28.png"></p></li></ol><p><strong>4.Accommodating Transformer onto FPGA 【GLSVLSI 2021】</strong><br><code>Transformer</code>在许多自然语言处理 (NLP) 任务中表现出色。然而，<code>Transformer</code> 的计算量和内存占用很大，因此很难部署在嵌入式设备上。现场可编程门阵列 (FPGA) 因其优势而被广泛用于加速深度学习算法。然而，经过训练的 <code>Transformer</code> 模型太大，无法放在FPGA上执行。为了将 <code>Transformer</code>放入 FPGA 并实现高效的执行，论文作者提出了一个加速框架，将算法级别的平衡模型压缩和硬件级别的 FPGA 实现优化相结合。在算法层面，采用块平衡剪枝，为这种剪枝技术提出了一种有效的稀疏矩阵存储格式，称为压缩块行(CBR)。在硬件层面，设计了一个稀疏模型的加速器。</p><ol><li><p><strong>CBR</strong><br>对于硬件友好的权重修剪，论文作者研究了两种常用的技术，块平衡修剪(BBP)和块修剪(BW)。BW 将子矩阵（例如 2 × 2）作为一个块，并选择块内权重的最大幅度或平均幅度作为整个块的代表。如果代表性幅度小于预定义的阈值，则整个块将被修剪。BBP将权重矩阵划分为块，并在每个块中进行行&#x2F;列修剪。他们在<code>Transformer</code>模型上实现了这两种技术，并测量了不同稀疏率下的准确性。结果显示，在几乎所有稀疏比率下，BBP 都比 BW 实现了更好的准确度。原因是 BW 删除了整个块（几个连续的行和列），它可能会修剪一些导致模型信息丢失的重要行和列，即使块大小很小。对于 BBP，它只选择修剪几个不重要的行或列（不连续）。因此，与 BW 相比，BBP 更细粒度，可以保持更重要的信息。最后，作者选择 BBP 作为他们的剪枝方案，并对此提出压缩块行(CBR)这种有效的稀疏矩阵存储格式。<br>两种剪枝方案和效果对比如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-06%2010-33-12.png"><br>所提出的CBR格式利用了块的平衡特性，在计算时节省了解码开销。CBR格式使用两个数组来表示块平衡稀疏矩阵。首先，使用三维数组来存储稀疏权重矩阵的非零元素。它的第一个维度用于记录块的索引。并且，每个块的非零值被连接（行主顺序），并存储在数组的第二和第三维中。第二个数组 (W-Index) 用于存储非零子行的块内部索引。它的第一个维度用于记录块的信息，第二个维度是记录非零子行的索引。CBR 格式可以显着减少内存使用，并且对于计算是有效的。<br>CBR格式的图示如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-06%2010-35-28.png"></p></li><li><p><strong>SpMatMul</strong><br>因为使用块平衡的剪枝，为了高效的执行矩阵乘法，使用<code>SpMatMul</code>代替<code>DenMatMul</code>,相关算法如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-06%2010-40-51.png"></p></li></ol><p>论文作者也给出了他们的<code>Transformer</code>加速器架构和具体的dataflow，其中的<code>Computation Engine</code>中使用的就是<code>SpMatMul</code>,加速器架构及具体的dataflow如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-06%2010-45-13.png"></p><p><strong>5.Accelerating Transformer Networks through Recomposing Softmax Layers【IISWC 2022】</strong></p><p><code>Transformer</code>模型已成为一个关键的深度学习模型，因为它提供了优于传统模型在图像处理、基因组学和自然语言处理等各个领域的准确性。最近，随着越来越多的 <code>Transformer</code> 模型处理更长的序列以提高其准确性率，softmax 层的重要性有所提高。然而，由于 softmax 层和其他相邻层的数据访问模式的差异，很难用先前研究中引入的方法加速 softmax 层。我们通过重组策略加速 softmax 层来解决这一挑战。通过将 softmax 层分解为多个子层，我们更改其数据访问模式。然后，我们将分解的 softmax 子层与后续和前面的操作融合。<code>softmax重组</code>通过显著减少片外内存流量，在现代 GPU 上推断 BERT、GPT-Neo、BigBird 和 Longformer 实现了高达 1.25×、1.12×、1.57 倍和 1.65 倍的加速。</p><p><strong>Matmul和softmax在GPU上的访存模式</strong>如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-06%2012-42-40.png"></p><ol><li><p><strong>Softmax Decomposition</strong><br>作者将 softmax 层分解为三个子层：Local Softmax (LS)、Inter-sub-vector Reduction (IR) 和 Global Scaling (GS)。softmax计算首先检索行向量中的所有元素，以获得归一化所需的每一行向量最大值和归一化因子，它对行向量的元素施加了严格的依赖关系。论文中通过最初在单个行向量拆分的每个子向量中执行操作，然后在整个行向量中归一化值来缓解这种严格的数据依赖问题。设子向量的大小为 $T$，行向量中的子向量数为 $N_{sv}$，其中 $N_{sv} &#x3D; L&#x2F;T$，子向量的第 $k$ 个子向量和第 $k$ 个结果相应地为 $X_k &#x3D; (x_{k,0} , x_{k,1} ,。.., x_{k,T-1})$ 和 $Y_k &#x3D; (y_{k,0} , y_{k,1} ,…, y_{k,T-1})$。<br>softmax decomposition的公式如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-06%2012-51-19.png"><br>其中:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-06%2012-55-43.png"><br>分解后的 softmax 在数学上的结果与原始 softmax 层的结果相同。<code>softmax分解</code>对每个子向量执行 softmax 操作，并利用为每个子向量定义的 $m’$ 和 $d’$ 作为中间值将其重构为原始 softmax 结果。我们根据上面的方程将 softmax 内核分解为 LS、IR 和 GS 内核。LS 内核进行 softmax 操作并产生 $m’$和 $d’$。IR 内核接收从 LS 内核计算的 $m’$和 $d’$作为输入，并执行归约以计算 $m$ 和 $d$。最后，GS 内核使用计算得到的$r’$缩放 LS 内核计算的中间值，以获得与原始 softmax 精确匹配的结果。<br>softmax decomposition的计算过程展示如下:<br><img src="/images/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E6%88%AA%E5%9B%BE%202024-01-06%2012-59-15.png"></p></li><li><p><strong>Softmax Fusion</strong><br><code>kernel融合</code>是一种将多个 GPU kernel聚合成一个kernel的方法。通常，内核将数据从片外存储器处理到共享内存或寄存器文件执行操作，并将结果存储回片外存储器中。如果前一个kernel的每个 TB(Thread Block) 的输出适合后一个内核的每个 TB 的输入，则可以通过融合消除两个相邻内核之间的片外内存访问。论文提出了 LS 子层与其前面的 MatMul 和 GS 子层与其随后的 MatMul 的融合。原始的 softmax 内核前面的操作产生softmax的输入，而后面的操作消耗softmax的输出,因此这是可行的。分解使 LS 和 GS 子层与其相邻层融合。通过将 LS kernel的 T 设置为等于 MatMul 内核的输出平铺宽度，LS kernel可以融合到其前面的 MatMul kernel中。类似地，如果通过在 GS kernel之后提供分配给 MatMul 内核 TB 的 $X’$ 对应的 $r’$，则 GS kernel可以很容易地与其随后的 MatMul 内核融合，则在 MatMul 操作之前执行逐元素缩放。由于分解后的 softmax kernel中的大多数片外内存访问都发生在 LS 和 GS kernel中，因此融合这两个kernel可以大大减少片外内存访问的数量。</p></li></ol><p><strong>6.ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention 【HPCA 2023】</strong></p><p><strong>7.Edge-MoE: Memory-Efficient Multi-Task Vision Transformer Architecture with Task-level Sparsity via Mixture-of-Experts 【ICCAD 2023】</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本科毕业设计的题目是&lt;code&gt;基于FPGA的通用Transformer加速器设计与优化&lt;/code&gt;,于大四上学期调研学习关于&lt;code&gt;</summary>
      
    
    
    
    <category term="学习总结" scheme="https://kzw3933.github.io/categories/%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="加速器设计" scheme="https://kzw3933.github.io/tags/%E5%8A%A0%E9%80%9F%E5%99%A8%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="transformer" scheme="https://kzw3933.github.io/tags/transformer/"/>
    
    <category term="量化" scheme="https://kzw3933.github.io/tags/%E9%87%8F%E5%8C%96/"/>
    
    <category term="论文阅读" scheme="https://kzw3933.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>使用Hexo和Github搭建个人博客</title>
    <link href="https://kzw3933.github.io/2023/12/27/%E4%BD%BF%E7%94%A8Hexo%E5%92%8CGithub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>https://kzw3933.github.io/2023/12/27/%E4%BD%BF%E7%94%A8Hexo%E5%92%8CGithub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</id>
    <published>2023-12-27T08:30:32.000Z</published>
    <updated>2024-01-04T03:44:54.488Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hexo介绍"><a href="#hexo介绍" class="headerlink" title="hexo介绍"></a>hexo介绍</h2><p>Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Heroku上，是搭建博客的首选框架。</p><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>个人电脑操作系统为<code>ubuntu22.04</code>,安装<code>nodejs</code>,<code>npm</code>,<code>git</code>,<code>hexo</code>如下:<br>先安装<code>nodejs</code>,<code>npm</code>和<code>git</code></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install nodejs</span><br><span class="line">sudo apt install npm</span><br><span class="line">sudo apt install git</span><br></pre></td></tr></table></figure><p>安装后的版本如下:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; node -v                             </span><br><span class="line">v12.22.9</span><br><span class="line">&gt;&gt;&gt; git --version</span><br><span class="line">git version 2.34.1</span><br><span class="line">&gt;&gt;&gt; npm -v                              </span><br><span class="line">8.5.1</span><br></pre></td></tr></table></figure><p>配置npm镜像<br>由于需要使用npm下载各种模块，默认从国外服务器下载，速度较慢,因此可以配置国内镜像,以下配置淘宝镜像(镜像地址可能会更换)</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm config <span class="built_in">set</span> registry https://registry.npmmirror.com/</span><br></pre></td></tr></table></figure><p>安装hexo</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure><p>安装后的hexo版本如下:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; hexo -v</span><br><span class="line">hexo-cli: 4.3.1</span><br><span class="line">os: linux 6.2.0-39-generic Ubuntu 22.04.3 LTS 22.04.3 LTS (Jammy Jellyfish)</span><br><span class="line">node: 12.22.9</span><br><span class="line">v8: 7.8.279.23-node.56</span><br><span class="line">uv: 1.43.0</span><br><span class="line">zlib: 1.2.11</span><br><span class="line">brotli: 1.0.9</span><br><span class="line">ares: 1.18.1</span><br><span class="line">modules: 72</span><br><span class="line">nghttp2: 1.43.0</span><br><span class="line">napi: 8</span><br><span class="line">llhttp: 2.1.6</span><br><span class="line">http<span class="emphasis">_parser: 2.9.4</span></span><br><span class="line"><span class="emphasis">openssl: 1.1.1m</span></span><br><span class="line"><span class="emphasis">cldr: 40.0</span></span><br><span class="line"><span class="emphasis">icu: 70.1</span></span><br><span class="line"><span class="emphasis">tz: 2023c</span></span><br><span class="line"><span class="emphasis">unicode: 14.0</span></span><br></pre></td></tr></table></figure><p>配置git及github的SSH密钥<br>在本地使用git提交文件到github仓库时，每次都需要输入账户密码进行账户验证，配置ssh登陆的公钥后，可以自动进行验证，无需每次进行账户的验证</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name <span class="string">&quot;用户名&quot;</span></span><br><span class="line">git config --global user.email <span class="string">&quot;邮箱&quot;</span></span><br><span class="line">ssh-keygen -t rsa -C <span class="string">&quot;邮箱&quot;</span></span><br></pre></td></tr></table></figure><p>使用ssh-keygen生成rsa密钥后会在用户家目录下(ubuntu shell中可使用<code>cd ~</code>进入)的.ssh文件夹下生成<code>id_rsa.pub</code>公钥文件，手动复制其中的内容，登陆github个人账户，在设置中的<code>SSH and GPG keys</code>配置SSH密钥</p><p>配置好ssh密钥后，可以使用命令行登陆github个人账户验证成功配置ssh登陆</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; ssh -T git@github.com</span><br><span class="line">Hi kzw3933! You&#x27;ve successfully authenticated, but GitHub does not provide shell access.</span><br></pre></td></tr></table></figure><p>需要注意的是，使用ssh密钥进行git验证时，如果使用vpn会失败,这时可以暂时关闭vpn(这时不使用vpn也可以正常拉取或推送仓库)</p><h2 id="创建Hexo个人博客"><a href="#创建Hexo个人博客" class="headerlink" title="创建Hexo个人博客"></a>创建Hexo个人博客</h2><p>使用以下命令在当前目录下新建一个名为<code>kzw-blogs</code>的目录作为个人博客的目录</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init kzw-blogs</span><br></pre></td></tr></table></figure><p>此时<code>kzw-blogs</code>内容如下(隐藏文件或文件夹还有<code>.github</code>和<code>.gitignore</code>):</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="emphasis">_config.landscape.yml         主题的配置文件  </span></span><br><span class="line"><span class="emphasis">- _</span>config.yml                   博客的配置文件</span><br><span class="line"><span class="bullet">-</span> node<span class="emphasis">_modules                  依赖包</span></span><br><span class="line"><span class="emphasis">- package.json</span></span><br><span class="line"><span class="emphasis">- package-lock.json             项目名称、描述、版本、运行和开发等信息</span></span><br><span class="line"><span class="emphasis">- scaffolds                     生成文章的一些模板</span></span><br><span class="line"><span class="emphasis">- source                        用来存放文章</span></span><br><span class="line"><span class="emphasis">- themes                        主题</span></span><br><span class="line"><span class="emphasis"></span></span><br></pre></td></tr></table></figure><p>接下来可在个人博客目录下输入<code>hexo server</code>或<code>hexo s</code>启动项目,在浏览器输入<code>http://localhost:4000/</code>,即可看到初始化的hexo博客</p><h2 id="将静态博客挂载到Github-Pages"><a href="#将静态博客挂载到Github-Pages" class="headerlink" title="将静态博客挂载到Github Pages"></a>将静态博客挂载到Github Pages</h2><p>前面搭建的博客，只能在本地访问，需要将这个博客部署到公共可访问的服务器上，才能被他人访问，这里使用github提供的Github Pages的功能 </p><p>这里首先要求在github创建一个名为<code>github账户名.github.io</code>的个人仓库</p><p>安装<code>hexo-deployer-git</code>用于部署</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>修改<code>_config.yml</code>文件,<code>_config.yml</code>是整个Hexo框架的配置文件。可以在里面修改大部分的配置(详细可参考官方的配置描述),这里只修改有关<code>Deployment</code>配置如下:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/one-command-deployment</span></span><br><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repository:</span> <span class="string">git@github.com:kzw3933/kzw3933.github.io.git</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">main</span></span><br></pre></td></tr></table></figure><p>需要将仓库配置成个人github仓库的地址</p><p>修改好配置后,运行以下命令将代码部署到github仓库</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo generate &amp;&amp; hexo deploy</span><br></pre></td></tr></table></figure><p>这里的<code>hexo clean</code>用于删除之前生成的文件，若未生成过静态文件，可忽略此命令;<code>hexo generate</code>用于生成静态文章，可以用<code>hexo g</code>缩写;<code>hexo deploy</code>用于部署文章，可以用<code>hexo d</code>缩写</p><p>稍等两分钟，打开浏览器访问<code>https://kzw3933.github.io</code>(换成个人的仓库地址)，这时候就可以看到博客内容了</p><h2 id="安装butterfly主题"><a href="#安装butterfly主题" class="headerlink" title="安装butterfly主题"></a>安装butterfly主题</h2><p>官方网站<code>https://butterfly.js.org/</code></p><p>在hexo博客根目录下,安装butterfly主题</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/butterfly</span><br></pre></td></tr></table></figure><p>应用主题需要修改<code>_config.yml</code>中的theme配置如下:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">theme:</span> <span class="string">butterfly</span></span><br></pre></td></tr></table></figure><p>安装一些依赖</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-renderer-jade hexo-generator-feed hexo-generator-sitemap hexo-browsersync hexo-generator-archive</span><br></pre></td></tr></table></figure><p>重新生成博客静态文件并推送</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo generate &amp;&amp; hexo deploy</span><br></pre></td></tr></table></figure><h2 id="页面配置"><a href="#页面配置" class="headerlink" title="页面配置"></a>页面配置</h2><h3 id="创建标签页"><a href="#创建标签页" class="headerlink" title="创建标签页"></a>创建标签页</h3><p>进入hexo博客根目录</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page tags</span><br></pre></td></tr></table></figure><p>此时，在<code>source</code>文件夹下，出现一个新的文件夹<code>tags</code>,里面有个<code>index.md</code>文件,修改如下:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 标签</span><br><span class="line">date: 2023-12-27 18:07:11</span><br><span class="line">type: &quot;tags&quot;</span><br><span class="line">orderby: random</span><br><span class="line"><span class="section">order: 1</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></table></figure><h3 id="创建分类页"><a href="#创建分类页" class="headerlink" title="创建分类页"></a>创建分类页</h3><p>进入hexo博客根目录</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page categories</span><br></pre></td></tr></table></figure><p>此时，在<code>source</code>文件夹下，出现一个新的文件夹<code>categories</code>,里面有个<code>index.md</code>文件,修改如下:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 分类</span><br><span class="line">date: 2023-12-27 18:11:51</span><br><span class="line"><span class="section">type: &quot;categories&quot;</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></table></figure><h3 id="友情链接页"><a href="#友情链接页" class="headerlink" title="友情链接页"></a>友情链接页</h3><p>进入hexo博客根目录</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page <span class="built_in">link</span></span><br></pre></td></tr></table></figure><p>此时，在<code>source</code>文件夹下，出现一个新的文件夹<code>link</code>,里面有个<code>index.md</code>文件,修改如下:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 友情链接</span><br><span class="line">date: 2023-12-27 18:43:05</span><br><span class="line"><span class="section">type: &quot;link&quot;</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></table></figure><p>要想显示外链，还需要在<code>source</code>文件夹下创建一个<code>_data/link.yml</code>,在<code>link.yml</code>中添加要链接的外链如下:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">class_name:</span> <span class="string">友情链接</span></span><br><span class="line">  <span class="attr">link_list:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Hexo</span></span><br><span class="line">      <span class="attr">link:</span> <span class="string">https://hexo.io/zh-tw/</span></span><br><span class="line">      <span class="attr">avatar:</span> <span class="string">https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg</span></span><br><span class="line">      <span class="attr">descr:</span> <span class="string">快速、简单且强大的博客框架</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">class_name:</span> <span class="string">网站</span></span><br><span class="line">  <span class="attr">link_list:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Youtube</span></span><br><span class="line">      <span class="attr">link:</span> <span class="string">https://www.youtube.com/</span></span><br><span class="line">      <span class="attr">avatar:</span> <span class="string">https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png</span></span><br><span class="line">      <span class="attr">descr:</span> <span class="string">视频网站</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Weibo</span></span><br><span class="line">      <span class="attr">link:</span> <span class="string">https://www.weibo.com/</span></span><br><span class="line">      <span class="attr">avatar:</span> <span class="string">https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png</span></span><br><span class="line">      <span class="attr">descr:</span> <span class="string">中国最大的社交分享平台</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Twitter</span></span><br><span class="line">      <span class="attr">link:</span> <span class="string">https://twitter.com/</span></span><br><span class="line">      <span class="attr">avatar:</span> <span class="string">https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png</span></span><br><span class="line">      <span class="attr">descr:</span> <span class="string">社交分享平台</span></span><br></pre></td></tr></table></figure><h3 id="404页面"><a href="#404页面" class="headerlink" title="404页面"></a>404页面</h3><p>butterfly主题内置了一个简单的<code>404页面</code>,可在主题的配置文件<code>hexo博客根目录/themes/butterfly/_config.yml</code>中开启</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># A simple 404 page</span></span><br><span class="line"><span class="attr">error_404:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">subtitle:</span> <span class="string">&#x27;Page Not Found&#x27;</span></span><br><span class="line">  <span class="attr">background:</span> <span class="string">https://i.loli.net/2020/05/19/aKOcLiyPl2JQdFD.png</span></span><br></pre></td></tr></table></figure><p>这里需要注意的是，上面只是创建了各个页面，如果想要在顶栏显示这些(实际上可以通过自己修改访问链接导航到指定的页面)，还需要配置导航栏菜单</p><h2 id="主题配置"><a href="#主题配置" class="headerlink" title="主题配置"></a>主题配置</h2><h3 id="语言配置"><a href="#语言配置" class="headerlink" title="语言配置"></a>语言配置</h3><p>在根目录下的<code>_config.yml</code>中修改语言设置,<code>butterfly</code>主题支持三种语言，具体可见<code>themes/butterfly/languages</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Site</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">Hexo</span></span><br><span class="line"><span class="attr">subtitle:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="attr">keywords:</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">John</span> <span class="string">Doe</span></span><br><span class="line"><span class="attr">language:</span> <span class="string">zh-CN</span></span><br><span class="line"><span class="attr">timezone:</span> <span class="string">&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="导航栏配置"><a href="#导航栏配置" class="headerlink" title="导航栏配置"></a>导航栏配置</h3><p>修改<code>butterfly</code>配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Menu 目錄</span></span><br><span class="line"><span class="attr">menu:</span></span><br><span class="line"><span class="string">首页:</span> <span class="string">/</span> <span class="string">||</span> <span class="string">fas</span> <span class="string">fa-home</span></span><br><span class="line"><span class="string">时间轴:</span> <span class="string">/archives/</span> <span class="string">||</span> <span class="string">fas</span> <span class="string">fa-archive</span></span><br><span class="line"><span class="string">标签:</span> <span class="string">/tags/</span> <span class="string">||</span> <span class="string">fas</span> <span class="string">fa-tags</span></span><br><span class="line"><span class="string">分类:</span> <span class="string">/categories/</span> <span class="string">||</span> <span class="string">fas</span> <span class="string">fa-folder-open</span></span><br><span class="line"><span class="comment"># 清单||fa fa-heartbeat:</span></span><br><span class="line"><span class="comment">#   音乐: /music/ || fas fa-music</span></span><br><span class="line"><span class="comment">#   照片: /Gallery/ || fas fa-images</span></span><br><span class="line"><span class="comment">#   电影: /movies/ || fas fa-video</span></span><br><span class="line"><span class="string">友链:</span> <span class="string">/link/</span> <span class="string">||</span> <span class="string">fas</span> <span class="string">fa-link</span></span><br><span class="line"><span class="comment"># 关于: /about/ || fas fa-heart</span></span><br></pre></td></tr></table></figure><h3 id="配置代码块显示"><a href="#配置代码块显示" class="headerlink" title="配置代码块显示"></a>配置代码块显示</h3><p>修改<code>butterfly</code>配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Code Blocks (代碼相關)</span></span><br><span class="line"><span class="comment"># --------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="attr">highlight_theme:</span> <span class="string">mac</span> <span class="string">light</span> <span class="comment">#  darker / pale night / light / ocean / mac / mac light / false</span></span><br><span class="line"><span class="attr">highlight_copy:</span> <span class="literal">true</span> <span class="comment"># copy button</span></span><br><span class="line"><span class="attr">highlight_lang:</span> <span class="literal">true</span> <span class="comment"># show the code language</span></span><br><span class="line"><span class="attr">highlight_shrink:</span> <span class="string">none</span> <span class="comment"># true: shrink the code blocks / false: expand the code blocks | none: expand code blocks and hide the button</span></span><br><span class="line"><span class="attr">highlight_height_limit:</span> <span class="number">200</span> <span class="comment"># unit: px</span></span><br><span class="line"><span class="attr">code_word_wrap:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h3 id="头像"><a href="#头像" class="headerlink" title="头像"></a>头像</h3><p>修改<code>butterfly</code>配置文件,将头像放置到<code>butterfly</code>主题文件夹下的<code>source/img</code>中</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Avatar (頭像)</span></span><br><span class="line"><span class="attr">avatar:</span></span><br><span class="line">  <span class="attr">img:</span> <span class="string">/img/avatar.png</span></span><br><span class="line">  <span class="attr">effect:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><h3 id="本地搜索"><a href="#本地搜索" class="headerlink" title="本地搜索"></a>本地搜索</h3><p>安装<code>hexo-generator-search</code></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-search --save</span><br></pre></td></tr></table></figure><p>修改<code>butterfly</code>配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">local_search:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Preload the search data when the page loads.</span></span><br><span class="line">  <span class="attr">preload:</span> <span class="literal">false</span></span><br><span class="line">  <span class="comment"># Show top n results per article, show all results by setting to -1</span></span><br><span class="line">  <span class="attr">top_n_per_article:</span> <span class="number">1</span></span><br><span class="line">  <span class="comment"># Unescape html strings to the readable one.</span></span><br><span class="line">  <span class="attr">unescape:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">CDN:</span></span><br></pre></td></tr></table></figure><h3 id="自定义主题色"><a href="#自定义主题色" class="headerlink" title="自定义主题色"></a>自定义主题色</h3><p>修改<code>butterfly</code>配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">theme_color:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">main:</span> <span class="string">&quot;#49B1F5&quot;</span></span><br><span class="line">  <span class="attr">paginator:</span> <span class="string">&quot;#00c4b6&quot;</span></span><br><span class="line">  <span class="attr">button_hover:</span> <span class="string">&quot;#FF7242&quot;</span></span><br><span class="line">  <span class="attr">text_selection:</span> <span class="string">&quot;#00c4b6&quot;</span></span><br><span class="line">  <span class="attr">link_color:</span> <span class="string">&quot;#99a9bf&quot;</span></span><br><span class="line">  <span class="attr">meta_color:</span> <span class="string">&quot;#858585&quot;</span></span><br><span class="line">  <span class="attr">hr_color:</span> <span class="string">&quot;#A4D8FA&quot;</span></span><br><span class="line">  <span class="attr">code_foreground:</span> <span class="string">&quot;#F47466&quot;</span></span><br><span class="line">  <span class="attr">code_background:</span> <span class="string">&quot;rgba(27, 31, 35, .05)&quot;</span></span><br><span class="line">  <span class="attr">toc_color:</span> <span class="string">&quot;#00c4b6&quot;</span></span><br><span class="line">  <span class="attr">blockquote_padding_color:</span> <span class="string">&quot;#49b1f5&quot;</span></span><br><span class="line">  <span class="attr">blockquote_background_color:</span> <span class="string">&quot;#49b1f5&quot;</span></span><br><span class="line">  <span class="attr">scrollbar_color:</span> <span class="string">&quot;#49b1f5&quot;</span></span><br></pre></td></tr></table></figure><h3 id="页面美化"><a href="#页面美化" class="headerlink" title="页面美化"></a>页面美化</h3><p>修改<code>butterfly</code>配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">beautify:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">field:</span> <span class="string">site</span> <span class="comment"># site/post</span></span><br><span class="line">  <span class="attr">title-prefix-icon:</span> <span class="string">&#x27;\f0c1&#x27;</span></span><br><span class="line">  <span class="attr">title-prefix-icon-color:</span> <span class="string">&quot;#F47466&quot;</span></span><br></pre></td></tr></table></figure><h3 id="字数统计"><a href="#字数统计" class="headerlink" title="字数统计"></a>字数统计</h3><p>安装<code>hexo-wordcount</code></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-wordcount --save</span><br></pre></td></tr></table></figure><p>修改<code>butterfly</code>配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">wordcount:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">post_wordcount:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">min2read:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">total_wordcount:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h3 id="展示数学公式"><a href="#展示数学公式" class="headerlink" title="展示数学公式"></a>展示数学公式</h3><p>安装<code>hexo-renderer-kramed</code></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure><p>修改<code>butterfly</code>配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MathJax</span></span><br><span class="line"><span class="attr">mathjax:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">per_page:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h3 id="社交图标"><a href="#社交图标" class="headerlink" title="社交图标"></a>社交图标</h3><p>修改<code>butterfly</code>配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Social Settings (社交圖標設置)</span></span><br><span class="line"><span class="attr">formal:</span></span><br><span class="line">  <span class="attr">icon:</span> <span class="string">link</span> <span class="string">||</span> <span class="string">the</span> <span class="string">description</span> <span class="string">||</span> <span class="string">color</span></span><br><span class="line"><span class="attr">social:</span></span><br><span class="line">  <span class="attr">fab fa-github:</span> <span class="string">https://github.com/kzw3933</span> <span class="string">||</span> <span class="string">Github</span> <span class="string">||</span> <span class="string">&#x27;#24292e&#x27;</span></span><br><span class="line">  <span class="attr">fas fa-envelope:</span> <span class="string">mailto:3099097649@qq.com</span> <span class="string">||</span> <span class="string">Email</span> <span class="string">||</span> <span class="string">&#x27;#4a7dbe&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="顶部图"><a href="#顶部图" class="headerlink" title="顶部图"></a>顶部图</h3><p>修改<code>butterfly</code>配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If the banner of page not setting, it will show the top_img</span></span><br><span class="line"><span class="attr">default_top_img:</span> <span class="string">/img/top.jpg</span></span><br></pre></td></tr></table></figure><h3 id="网站背景"><a href="#网站背景" class="headerlink" title="网站背景"></a>网站背景</h3><p>修改<code>butterfly</code>配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Website Background (設置網站背景)</span></span><br><span class="line"><span class="comment"># can set it to color or image (可設置圖片 或者 顔色)</span></span><br><span class="line"><span class="comment"># The formal of image: url(http://xxxxxx.com/xxx.jpg)</span></span><br><span class="line"><span class="attr">background:</span> <span class="string">&quot;#FFFFFF&quot;</span></span><br></pre></td></tr></table></figure><h3 id="footer背景"><a href="#footer背景" class="headerlink" title="footer背景"></a>footer背景</h3><p>修改<code>butterfly</code>配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Footer Background</span></span><br><span class="line"><span class="attr">footer_bg:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="针对个人博客的配置"><a href="#针对个人博客的配置" class="headerlink" title="针对个人博客的配置"></a>针对个人博客的配置</h2><p>修改hexo博客根目录的配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Site</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">kzw3933&#x27;s</span> <span class="string">个人博客</span></span><br><span class="line"><span class="attr">subtitle:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">&#x27;记录个人的学习笔记和感悟&#x27;</span></span><br><span class="line"><span class="attr">keywords:</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">kzw3933</span></span><br><span class="line"><span class="attr">language:</span> <span class="string">zh-CN</span></span><br><span class="line"><span class="attr">timezone:</span> <span class="string">&#x27;Asia/Shanghai&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># URL</span></span><br><span class="line"><span class="comment">## Set your site url here. For example, if you use GitHub Page, set url as &#x27;https://username.github.io/project&#x27;</span></span><br><span class="line"><span class="attr">url:</span> <span class="string">https://kzw3933.github.io/</span></span><br><span class="line"><span class="attr">permalink:</span> <span class="string">:year/:month/:day/:title/</span></span><br><span class="line"><span class="attr">permalink_defaults:</span></span><br><span class="line"><span class="attr">pretty_urls:</span></span><br><span class="line">  <span class="attr">trailing_index:</span> <span class="literal">true</span> <span class="comment"># Set to false to remove trailing &#x27;index.html&#x27; from permalinks</span></span><br><span class="line">  <span class="attr">trailing_html:</span> <span class="literal">true</span> <span class="comment"># Set to false to remove trailing &#x27;.html&#x27; from permalinks</span></span><br></pre></td></tr></table></figure><p>修改<code>butterfly</code>配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">card_author:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">description:</span></span><br><span class="line">    <span class="attr">button:</span></span><br><span class="line">      <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">icon:</span> <span class="string">fab</span> <span class="string">fa-github</span></span><br><span class="line">      <span class="attr">text:</span> <span class="string">Follow</span> <span class="string">Me</span></span><br><span class="line">      <span class="attr">link:</span> <span class="string">https://github.com/kzw3933</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;hexo介绍&quot;&gt;&lt;a href=&quot;#hexo介绍&quot; class=&quot;headerlink&quot; title=&quot;hexo介绍&quot;&gt;&lt;/a&gt;hexo介绍&lt;/h2&gt;&lt;p&gt;Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub</summary>
      
    
    
    
    <category term="笔记教程" scheme="https://kzw3933.github.io/categories/%E7%AC%94%E8%AE%B0%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="博客搭建" scheme="https://kzw3933.github.io/tags/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
  </entry>
  
</feed>
