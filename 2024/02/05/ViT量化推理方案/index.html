<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>ViT量化推理方案 | kzw3933's 个人博客</title><meta name="author" content="kzw3933"><meta name="copyright" content="kzw3933"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="ViT架构 由上图可知，ViT 由 Patch Embedding, Transformer Encoder 和 MLP Head 三部分组成。具体如下：  Patch Embedding: 一层 Conv2d 并加上位置编码, 其中位置编码在实现时作为可学习参数参与训练获得  Transformer Encoder: 具体结构如上图，由L个 Block 组成，每个Block 则由 一个带跳连结构">
<meta property="og:type" content="article">
<meta property="og:title" content="ViT量化推理方案">
<meta property="og:url" content="https://kzw3933.github.io/2024/02/05/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/index.html">
<meta property="og:site_name" content="kzw3933&#39;s 个人博客">
<meta property="og:description" content="ViT架构 由上图可知，ViT 由 Patch Embedding, Transformer Encoder 和 MLP Head 三部分组成。具体如下：  Patch Embedding: 一层 Conv2d 并加上位置编码, 其中位置编码在实现时作为可学习参数参与训练获得  Transformer Encoder: 具体结构如上图，由L个 Block 组成，每个Block 则由 一个带跳连结构">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://kzw3933.github.io/img/avatar.png">
<meta property="article:published_time" content="2024-02-05T06:16:38.000Z">
<meta property="article:modified_time" content="2024-03-06T07:05:04.725Z">
<meta property="article:author" content="kzw3933">
<meta property="article:tag" content="毕业设计">
<meta property="article:tag" content="加速器">
<meta property="article:tag" content="ViT">
<meta property="article:tag" content="量化">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kzw3933.github.io/img/avatar.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://kzw3933.github.io/2024/02/05/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'ViT量化推理方案',
  isPost: true,
  isHome: false,
  isHighlightShrink: undefined,
  isToc: true,
  postUpdate: '2024-03-06 15:05:04'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/atom.xml" title="kzw3933's 个人博客" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/top.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="kzw3933's 个人博客"><span class="site-name">kzw3933's 个人博客</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">ViT量化推理方案</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-02-05T06:16:38.000Z" title="发表于 2024-02-05 14:16:38">2024-02-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-06T07:05:04.725Z" title="更新于 2024-03-06 15:05:04">2024-03-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/">学习总结</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>14分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="ViT量化推理方案"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="ViT架构"><a href="#ViT架构" class="headerlink" title="ViT架构"></a>ViT架构</h2><p><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205095525415.png"></p>
<p>由上图可知，ViT 由 Patch Embedding, Transformer Encoder 和 MLP Head 三部分组成。具体如下：</p>
<ul>
<li><p>Patch Embedding: 一层 Conv2d 并加上位置编码, 其中位置编码在实现时作为可学习参数参与训练获得</p>
</li>
<li><p>Transformer Encoder: 具体结构如上图，由L个 Block 组成，每个Block 则由 一个带跳连结构的 MHA 和 一个带跳连结构的 MLP 组成，同时在 MHA 和 MLP 之前需进行 LN 操作。MHA 和 MLP 主要是由矩阵乘积实现(MHA中还包含softmax操作, MLP中包含激活函数GELU操作, 这些运算是非线性的)。由于Transformer Encoder Block是ViT中的核心部分，也是量化的核心部分，因此下面对其操作进行形式化叙述。</p>
<ul>
<li><p>每个Block的计算过程可形式化表示如下:</p>
<p>$$<br>\begin{aligned} \boldsymbol{Y}<em>{l-1} &amp; &#x3D;\operatorname{MHA}\left(\operatorname{LayerNorm}\left(\boldsymbol{X}</em>{l-1}\right)\right)+\boldsymbol{X}<em>{l-1} \ \boldsymbol{X}<em>l &amp; &#x3D;\operatorname{MLP}\left(\operatorname{LayerNorm}\left(\boldsymbol{Y}</em>{l-1}\right)\right)+\boldsymbol{Y}</em>{l-1}\end{aligned}<br>$$</p>
<p>其中$\boldsymbol{X}<em>{l-1}$是上一层Block的输出(或者Patch Embedding的输出), $\boldsymbol{Y}</em>{l-1}$是MHA的输出，同时作为MLP的输入，$\boldsymbol{X}_{l}$​则是MLP的输出，也是该Block的输出。</p>
</li>
<li><p>MHA可形式化表示如下:</p>
<p>$$<br>\begin{aligned} {\left[\boldsymbol{Q}_i, \boldsymbol{K}_i, \boldsymbol{V}_i\right] } &amp; &#x3D;\boldsymbol{X}^{\prime} \boldsymbol{W}^{q k v}+\boldsymbol{b}^{q k v} \quad i&#x3D;1,2, \cdots, h \ \operatorname{Attn}_i &amp; &#x3D;\operatorname{Softmax}\left(\frac{\boldsymbol{Q}_i \cdot \boldsymbol{K}_i^T}{\sqrt{D_h}}\right) \boldsymbol{V}_i \ \operatorname{MHA}\left(\boldsymbol{X}^{\prime}\right) &amp; &#x3D;\left[\operatorname{Attn}_1, \operatorname{Attn}_2, \ldots, \operatorname{Attn}_h\right] \boldsymbol{W}^o+\boldsymbol{b}^o\end{aligned}<br>$$</p>
<p>对MHA的输入进行投影获得对应的$\boldsymbol{Q}$、$\boldsymbol{K}$、$\boldsymbol{V}$,同时在嵌入维度上进行分组获得各个head的$\boldsymbol{Q}$、$\boldsymbol{K}$、$\boldsymbol{V}$,也即$\boldsymbol{Q}_i$, $\boldsymbol{K}_i$, $\boldsymbol{V}_i$, 接着各个head对应的$\boldsymbol{Q}$、$\boldsymbol{K}$运算并经过softmax运算获得各个head的score，将score和$\boldsymbol{V}$相乘获得各个head的$\boldsymbol{Attn}$,最后将各个head的$\boldsymbol{Attn}$​拼接并做投影获得输出结果。</p>
</li>
<li><p>MLP可形式化表示如下:</p>
<p>$$<br>\operatorname{MLP}\left(\boldsymbol{Y}^{\prime}\right)&#x3D;\operatorname{GELU}\left(\boldsymbol{Y}^{\prime} \boldsymbol{W}^1+\boldsymbol{b}^1\right) \boldsymbol{W}^2+\boldsymbol{b}^2<br>$$</p>
<p>MLP主要对输入进行两层线性层运算，在两层之间使用GELU作为激活函数。</p>
</li>
</ul>
</li>
<li><p>MLP Head: 一层线性层, 在ViT原论文中因为面向分类任务是用于获得属于各个类别的概率。</p>
</li>
</ul>
<h2 id="量化方案"><a href="#量化方案" class="headerlink" title="量化方案"></a>量化方案</h2><p>关于模型量化方案，需要考虑的是对哪些层进行量化，量化位宽的选择，量化方式的确定。下面依次进行介绍。</p>
<h3 id="量化层"><a href="#量化层" class="headerlink" title="量化层"></a>量化层</h3><p>如前面展示的ViT架构中的各种组件，其中有<strong>矩阵运算作为核心算子</strong>的MHA，MLP中的矩阵运算部分都可以进行量化，但是对于模型中的softmax, gelu，LN(也就是Norm)不进行量化操作，需要注意的是残差连接也不进行量化。 在量化和非量化部分之间需要插入量化节点和非量化节点。具体见下图:<br><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205141305562.png"></p>
<p>如图所示，在非线性层和残差连接以及最后的输出前进行了<strong>反量化操作</strong>,在<code>QConv2d</code>,<code>QLinear</code>和<code>QMatMul</code>的输入进行量化。</p>
<h3 id="量化位宽"><a href="#量化位宽" class="headerlink" title="量化位宽"></a>量化位宽</h3><p>为较好的保持模型的效果同时支持ViT在FPGA上的高效推理，这里对量化的权重和激活值采用常用的<code>uint8</code>形式。具体来说：对于线性层(<strong>矩阵运算作为核心算子</strong>)的网络层对其输入进行<code>uint8</code>量化，其输出由于一般进行汇总操作,为防止溢出，采用<code>int32</code>；对于非线性层(softmax 和 gelu) 由于无法直接使用量化后的数据进行推理，采用定点数形式(<strong>具体在FPGA上的小数位宽仍需分析这些层的输入输出分布确定</strong>)，也就是说，在前面层量化得到的整数(<strong>一般是<code>int32</code>,因为前面层的输出需要以<code>int32</code>存储</strong>)需要进行反量化后输入到这些非线性层。</p>
<h3 id="量化方式"><a href="#量化方式" class="headerlink" title="量化方式"></a>量化方式</h3><p>确定了量化的位宽，接下来，就需要考虑量化的方式了，而为了尽可能保持原有模型的精度，<strong>需要考虑模型推理过程中的数据分布来确定具体的量化方式</strong>。</p>
<p>量化的方法在CNN领域已经得到充分的研究，一般来说，采用<code>weight</code>进行<code>per-channel量化</code>,各层的输入采用<code>per-tensor</code>的量化，一般都使用Uniform的量化。但是同样的配置在以Transformer为基础的ViT上效果并不好，一些研究工作发现ViT推理过程中经过 <code>LN</code> 层后的输出具有严重的跨通道差异，因此在进入下一层之前采用<code>per-channel</code>量化可以保持模型较高的精度。然而，对这些<code>激活值</code>采用<code>per-channel量化</code>会给推理过程带来较大的负担。</p>
<p>在这里，决定采用 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.08254">RepQ-ViT</a> 论文中的量化方案，其核心思想是将 <code>LN</code> 层输出的跨通道差异通过仿射变换转移到下一层的<code>weight</code>中，而<code>weight</code>本身就需要进行<code>per-channel</code>的量化, 而且这些调整是在软件端就可完成的，也不会带来<code>FPGA</code>推理上的负担，这样就可以实现<code>激活值</code>的<code>per-tensor</code>量化, 这一步在论文中被称为<strong>Scale Reparam for LayerNorm Activations</strong>。除此之外，论文也提到经过<code>softmax</code>计算得到的<code>attention</code>分数具有幂率分布特点，因此采用<code>log量化</code>比较合适，但是使用传统的 $log2$ 量化 间距过大，因而采用  $log\sqrt{2}$ 量化 比较合适，但是 $log2$量化 在硬件实现上很高效，直接使用位运算即可，因此，论文根据 $log2$ 函数和 $log\sqrt{2}$函数的关系，通过分离出一个系数，使用 $log2$ 完成 $log\sqrt{2}$​,以达到硬件的高效实现, 这一步在论文中被称为<strong>Scale Reparam for Softmax Activations</strong>。具体的原理以下分别介绍。</p>
<h4 id="Scale-Reparam-for-LayerNorm-Activations"><a href="#Scale-Reparam-for-LayerNorm-Activations" class="headerlink" title="Scale Reparam for LayerNorm Activations"></a>Scale Reparam for LayerNorm Activations</h4><p>$$<br>\operatorname{LayerNorm}\left(\boldsymbol{X}<em>{n,:}\right)&#x3D;\frac{\boldsymbol{X}</em>{n,:}-\mathrm{E}\left[\boldsymbol{X}<em>{n,:}\right]}{\sqrt{\operatorname{Var}\left[\boldsymbol{X}</em>{n,:}\right]+\epsilon}} \odot \boldsymbol{\gamma}+\boldsymbol{\beta}<br>$$<br>其中 $n&#x3D;1,2, \cdots, N, \mathrm{E}\left[\boldsymbol{X}<em>{n,:}\right]$ 和 $\operatorname{Var}\left[\boldsymbol{X}</em>{n,:}\right]$ 分别是均值和方差, 而 $\gamma \in \mathbb{R}^D$ 和 $\boldsymbol{\beta} \in \mathbb{R}^D$ 则是代表线性仿射变换的参数因子的行向量。另外， $\odot$​ 代表 Hadamard product。</p>
<p>正如前面所说，论文中提到对于经过LN后的激活值，由于其具有严重的通道间差异，在量化时采用<code>per-channel</code>量化可在一定程度解决这个问题，但是对激活值进行<code>per-channel</code>的量化在硬件上效率不高。论文中采用首先进行<code>per-channel</code>的量化，接着将其转化为<code>per-tensor</code>的量化。</p>
<p>对于这些激活值的量化采用Uniform的量化，具体原理如下:<br>$$<br>\begin{aligned} &amp; \text { Quant: } \boldsymbol{x}^{(\mathbb{Z})}&#x3D;\operatorname{clip}\left(\left\lfloor\frac{\boldsymbol{x}}{s}\right\rceil+z, 0,2^b-1\right) \ &amp; \text { DeQuant : } \hat{\boldsymbol{x}}&#x3D;s\left(\boldsymbol{x}^{(\mathbb{Z})}-z\right) \approx \boldsymbol{x} \ &amp; s&#x3D;\frac{\max (\boldsymbol{x})-\min (\boldsymbol{x})}{2^b-1}, \quad z&#x3D;\left\lfloor-\frac{\min (\boldsymbol{x})}{s}\right\rceil \ &amp; \end{aligned}<br>$$</p>
<p>以上给出了对一个tensor $\boldsymbol{x}$进行uniform 的量化和反量化的数学原理，对于LayerNorm Activations，如果是<code>per-tensor</code>的量化则$\boldsymbol{x}$是LN的整个输出, 而<code>per-channel</code>则是输出按各个通道划分出的tensor。</p>
<p>先进行<code>per-channel</code>的量化，会得到对应于各个通道的缩放因子$s$和零点$z$, 而<code>per-tensor</code>则只有一个缩放因子$s$和零点$z$, 这里一个自然也是论文中的方法是, 设置其量化参数为：<code>scale</code>为各通道<code>scale</code>的均值,<code>zero-point</code>为各通道<code>zero-point</code>的均值。由此逆推，如何对LN输出的激活值进行仿射变换，使得其分布能符合这个量化参数，也就是说各个通道上的量化参数都是一样的，因此也就等价变成了<code>per-tensor</code>的量化， 这里推导仿射变换的参数如下:</p>
<p>$$<br>\begin{aligned} &amp; \tilde{\boldsymbol{z}}&#x3D;\boldsymbol{z}-\boldsymbol{r}<em>2&#x3D;\left\lfloor-\frac{\left[\min \left(\boldsymbol{X}</em>{:, d}^{\prime}\right)\right]<em>{1 \leq d \leq D}+\boldsymbol{s} \odot \boldsymbol{r}<em>2}{\boldsymbol{s}}\right\rfloor \ &amp; \tilde{\boldsymbol{s}}&#x3D;\frac{\boldsymbol{s}}{\boldsymbol{r}<em>1}&#x3D;\frac{\left[\max \left(\boldsymbol{X}</em>{:, d}^{\prime}\right)-\min \left(\boldsymbol{X}</em>{:, d}^{\prime}\right)\right]</em>{1 \leq d \leq D} &#x2F; \boldsymbol{r}_1}{2^b-1}\end{aligned}<br>$$</p>
<p>其中$\tilde{\boldsymbol{z}}$​和$\tilde{\boldsymbol{s}}$​分别代表$\boldsymbol{z}$​和$\boldsymbol{s}$​的均值。由于LN中也有进行仿射变换，因此可以将这里的$\boldsymbol{r1}$​和$\boldsymbol{r2}$​按如下公式合并到LN中的$\boldsymbol{\beta}$​和$\boldsymbol{\gamma}$​中。</p>
<p>$$<br>\widetilde{\boldsymbol{\beta}}&#x3D;\frac{\boldsymbol{\beta}+\boldsymbol{s} \odot \boldsymbol{r}_2}{\boldsymbol{r}_1}, \quad \tilde{\boldsymbol{\gamma}}&#x3D;\frac{\boldsymbol{\gamma}}{\boldsymbol{r}_1}<br>$$</p>
<p>当然，经过这样的仿射变换， LN输出激活值的分布发生了偏移，论文中采取的操作是对下一层的weight和bias进行仿射变换以抵消影响，因为weight本身就是进行<code>per-channel</code>量化的，因此量化方面不受影响。</p>
<p>$$<br>\begin{array}{r}\boldsymbol{X}<em>{n,:}^{\prime} \boldsymbol{W}</em>{:, j}^{q k v}+\boldsymbol{b}<em>j^{q k v}&#x3D;\frac{\boldsymbol{X}</em>{n,:}^{\prime}+\boldsymbol{s} \odot \boldsymbol{r}_2}{\boldsymbol{r}_1}\left(\boldsymbol{r}<em>1 \odot \boldsymbol{W}</em>{:, j}^{q k v}\right) \ +\left(\boldsymbol{b}_j^{q k v}-\left(\boldsymbol{s} \odot \boldsymbol{r}<em>2\right) \boldsymbol{W}</em>{:, j}^{q k v}\right)\end{array}<br>$$</p>
<p>$$<br>\begin{aligned} \widetilde{\boldsymbol{W}}_{:, j}^{q k v} &amp; &#x3D;\boldsymbol{r}<em>1 \odot \boldsymbol{W}</em>{:, j}^{q k v} \ \widetilde{\boldsymbol{b}}_j^{q k v} &amp; &#x3D;\boldsymbol{b}_j^{q k v}-\left(\boldsymbol{s} \odot \boldsymbol{r}<em>2\right) \boldsymbol{W}</em>{:, j}^{q k v}\end{aligned}<br>$$</p>
<p>以上给出了对下一层的weight和bias进行的仿射变换的参数推导。</p>
<h4 id="Scale-Reparam-for-Softmax-Activations"><a href="#Scale-Reparam-for-Softmax-Activations" class="headerlink" title="Scale Reparam for Softmax Activations"></a>Scale Reparam for Softmax Activations</h4><p>由于softmax后的激活值具有明显的幂律分布特点，使用均匀分布对它效果并不好，这里对它采用$log$形式的量化，量化原理如下:</p>
<p>$$<br>\begin{aligned} &amp; \text { Quant: } \boldsymbol{x}^{(\mathbb{Z})}&#x3D;\operatorname{clip}\left(\left\lfloor-\log _2 \frac{\boldsymbol{x}}{s}\right\rceil, 0,2^b-1\right) \ &amp; \text { DeQuant }: \hat{\boldsymbol{x}}&#x3D;s \cdot 2^{-\boldsymbol{x}^{(Z)}} \approx \boldsymbol{x}\end{aligned}<br>$$</p>
<p>值得注意的是，这里的$s$​实现时一般使用最大值或接近最大的百分位点对应的值(如95%位点对应的值)</p>
<p>论文的研究工作发现，使用$log2$​在实践中效果并不好，它提供的quantization resolution不够，量化点之间的间隔较大，而使用$log\sqrt{2}$​则能更准确的描述数据分布，但是$log\sqrt{2}$​无法在硬件上高效的实现，仿照LN激活值的Scale Reparam思想，对Softmax的激活值也进行Scale Reparam，使其转换成$log2$​量化，由于数字在硬件上都以二进制存储，对于$\left\lfloor\log _{2}\right\rceil$​操作可以直接使用其最高位表示获得(如$\left\lfloor\log _{2}34\right\rceil &#x3D; \left\lfloor\log _{2}(32+2)\right\rceil &#x3D; \left\lfloor\log _{2}(100010_2)\right\rceil &#x3D; log32 &#x3D; 5$​)。</p>
<p>由于$log\sqrt{2}$量化和$log2$量化有如下关系:</p>
<p>$$<br>\begin{aligned} \boldsymbol{A}^{(\mathbb{Z})} &amp; &#x3D;\operatorname{clip}\left(\left\lfloor-\log _{\sqrt{2}} \frac{\boldsymbol{A}}{s}\right\rceil, 0,2^b-1\right) \ &amp; &#x3D;\operatorname{clip}\left(\left\lfloor-2 \log _2 \frac{\boldsymbol{A}}{s}\right\rfloor, 0,2^b-1\right)\end{aligned}<br>$$</p>
<p>$$<br>\begin{aligned} \widehat{\boldsymbol{A}} &amp; &#x3D;s \cdot \sqrt{2}^{-\boldsymbol{A}^{(\mathbb{Z})}}&#x3D;s \cdot 2^{-\frac{A^{(\mathbb{Z})}}{2}} \ &amp; &#x3D; \begin{cases}s \cdot 2^{-\frac{A^{(Z)}}{2}} &amp; \boldsymbol{A}^{(\mathbb{Z})}&#x3D;2 k, k \in \mathbb{Z} \ s \cdot 2^{-\frac{\left.A^{(Z)}\right)+1}{2}} \cdot \sqrt{2} &amp; \boldsymbol{A}^{(\mathbb{Z})}&#x3D;2 k+1, k \in \mathbb{Z}\end{cases} \ &amp; &#x3D;s \cdot 2^{\left\lfloor\frac{\boldsymbol{A}^{(\mathbb{Z})}}{2}\right\rfloor} \cdot\left[\mathbb{1}\left(\boldsymbol{A}^{(\mathbb{Z})}\right) \cdot(\sqrt{2}-1)+1\right]\end{aligned}<br>$$</p>
<p>对于量化部分，乘上系数2再进行clip即可，而对于反量化则可以将$\mathbb{1}\left(\boldsymbol{A}^{(\mathbb{Z})}\right) \cdot(\sqrt{2}-1)+1$合并到系数$s$中，这个过程可以在软件端完成。</p>
<p>$$<br>\tilde{s}&#x3D;s \cdot\left[\mathbb{1}\left(\boldsymbol{A}^{(\mathbb{Z})}\right) \cdot(\sqrt{2}-1)+1\right]<br>$$</p>
<h2 id="量化细节-软件模拟"><a href="#量化细节-软件模拟" class="headerlink" title="量化细节(软件模拟)"></a>量化细节(软件模拟)</h2><p>具体的量化只涉及以矩阵运算作为核心算子的线性层，以下先分析<code>Linear</code>,<code>Matmul</code>和<code>Conv2d</code>的量化，然后分析量化层与非量化层之间的反量化操作。</p>
<h3 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h3><p>weight采用<code>per-channel</code>的uniform量化,对于input采用<code>per-tensor</code>的uniform量化,bias和输出一样采用<code>int32</code> 保存, 缩放因子采用 input 和 weight 的缩放因子乘积，量化的偏移为0。具体代码如下(<strong>freeze对weight和bias进行量化，这些可在推理前固化，输入量化的过程和量化推理过程在inference中体现</strong>):</p>
<p><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205113648932.png"></p>
<p><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205113629986.png"></p>
<h3 id="Matmul"><a href="#Matmul" class="headerlink" title="Matmul"></a>Matmul</h3><p>Matmul层出现在Attention计算中的$attn &#x3D; Matmul(K, Q)$ 和 $O &#x3D; Matmul(attn, V)$ ，对于 attn 计算的Matmul ，其输入都采用<code>uint8</code>的uniform量化，而对于 O 计算的Matmul，对attn 输入采用   $log\sqrt{2}$ 量化 ，对V输入采用<code>uint8</code>的uniform量化。(<strong>freeze对weight和bias进行量化，这些可在推理前固化，输入量化的过程和量化推理过程在inference中体现</strong>):</p>
<p><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205113900838.png"></p>
<p><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205113917325.png"></p>
<h3 id="Conv2d"><a href="#Conv2d" class="headerlink" title="Conv2d"></a>Conv2d</h3><p>这里与Linear一致，weight采用<code>per-channel</code>的uniform量化,对于input采用<code>per-tensor</code>的uniform量化,bias和输出一样采用<code>int32</code> 保存, 缩放因子采用 input 和 weight 的缩放因子乘积，量化的偏移为0。(<strong>freeze对weight和bias进行量化，这些可在推理前固化，输入量化的过程和量化推理过程在inference中体现</strong>):</p>
<p><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205113946125.png"></p>
<p><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205114011648.png"></p>
<h2 id="效果验证"><a href="#效果验证" class="headerlink" title="效果验证"></a>效果验证</h2><p>使用 timm 库中开源的<strong>ViT-tiny</strong>模型进行验证, 具体是使用自定义的<code>QConv2d</code>,<code>QLinear</code>,<code>QMatmul</code>提取原有模型被量化的权重参数，并通过前面的<code>inference</code>实现量化推理,供高层的<code>QAttention</code>,<code>QMLP</code>,<code>QVisionTransformer</code>等调实现ViT模型的量化推理(<strong>其中也涉及前面介绍量化推理过程中的在非线性算子前的反量化</strong>)。使用<strong>ImageNet-1k</strong>数据集进行验证，由于train部分数据集过大，下载较慢，直接使用验证集(val_images.tar.gz 6.7G)中的数据进行验证。效果如下:</p>
<p><img src="/images/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/image-20240205131532524.png"></p>
<p>经模拟，量化后的精度损失较小。</p>
<h2 id="后续改进"><a href="#后续改进" class="headerlink" title="后续改进"></a>后续改进</h2><p>计划尝试Softmax的量化推理方式以及GeLU函数的近似操作。 </p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://kzw3933.github.io">kzw3933</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://kzw3933.github.io/2024/02/05/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/">https://kzw3933.github.io/2024/02/05/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kzw3933.github.io" target="_blank">kzw3933's 个人博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/">毕业设计</a><a class="post-meta__tags" href="/tags/%E5%8A%A0%E9%80%9F%E5%99%A8/">加速器</a><a class="post-meta__tags" href="/tags/ViT/">ViT</a><a class="post-meta__tags" href="/tags/%E9%87%8F%E5%8C%96/">量化</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/01/30/%E4%BD%BF%E7%94%A8C%E4%B8%8ERust%E6%9E%84%E5%BB%BA%E9%9D%A2%E5%90%91%E7%BB%88%E7%AB%AF%E7%9A%84%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84%E4%BD%93%E9%AA%8C/" title="使用C与Rust构建面向终端的编辑器的体验"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">使用C与Rust构建面向终端的编辑器的体验</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E9%98%B6%E6%AE%B5%E6%80%A7%E6%80%BB%E7%BB%93-%E4%B8%80/" title="论文阅读阶段性总结(一)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-03</div><div class="title">论文阅读阶段性总结(一)</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">kzw3933</div><div class="author-info__description">记录个人的学习笔记和感悟</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/kzw3933"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/kzw3933" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:3099097649@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">kzw3933's 个人博客</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#ViT%E6%9E%B6%E6%9E%84"><span class="toc-number">1.</span> <span class="toc-text">ViT架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8F%E5%8C%96%E6%96%B9%E6%A1%88"><span class="toc-number">2.</span> <span class="toc-text">量化方案</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8F%E5%8C%96%E5%B1%82"><span class="toc-number">2.1.</span> <span class="toc-text">量化层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8F%E5%8C%96%E4%BD%8D%E5%AE%BD"><span class="toc-number">2.2.</span> <span class="toc-text">量化位宽</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8F%E5%8C%96%E6%96%B9%E5%BC%8F"><span class="toc-number">2.3.</span> <span class="toc-text">量化方式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Scale-Reparam-for-LayerNorm-Activations"><span class="toc-number">2.3.1.</span> <span class="toc-text">Scale Reparam for LayerNorm Activations</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Scale-Reparam-for-Softmax-Activations"><span class="toc-number">2.3.2.</span> <span class="toc-text">Scale Reparam for Softmax Activations</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8F%E5%8C%96%E7%BB%86%E8%8A%82-%E8%BD%AF%E4%BB%B6%E6%A8%A1%E6%8B%9F"><span class="toc-number">3.</span> <span class="toc-text">量化细节(软件模拟)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Linear"><span class="toc-number">3.1.</span> <span class="toc-text">Linear</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Matmul"><span class="toc-number">3.2.</span> <span class="toc-text">Matmul</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conv2d"><span class="toc-number">3.3.</span> <span class="toc-text">Conv2d</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%88%E6%9E%9C%E9%AA%8C%E8%AF%81"><span class="toc-number">4.</span> <span class="toc-text">效果验证</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%8E%E7%BB%AD%E6%94%B9%E8%BF%9B"><span class="toc-number">5.</span> <span class="toc-text">后续改进</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/02/05/ViT%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/" title="ViT量化推理方案">ViT量化推理方案</a><time datetime="2024-02-05T06:16:38.000Z" title="发表于 2024-02-05 14:16:38">2024-02-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/01/30/%E4%BD%BF%E7%94%A8C%E4%B8%8ERust%E6%9E%84%E5%BB%BA%E9%9D%A2%E5%90%91%E7%BB%88%E7%AB%AF%E7%9A%84%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84%E4%BD%93%E9%AA%8C/" title="使用C与Rust构建面向终端的编辑器的体验">使用C与Rust构建面向终端的编辑器的体验</a><time datetime="2024-01-30T13:41:11.000Z" title="发表于 2024-01-30 21:41:11">2024-01-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/01/07/Ubuntu(%E5%8F%8C%E7%B3%BB%E7%BB%9F)%E5%AE%89%E8%A3%85%E3%80%81%E7%BE%8E%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%97%A5%E5%B8%B8%E4%BD%BF%E7%94%A8/" title="Ubuntu(双系统)安装、美化以及日常使用">Ubuntu(双系统)安装、美化以及日常使用</a><time datetime="2024-01-07T01:55:55.000Z" title="发表于 2024-01-07 09:55:55">2024-01-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/01/04/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/" title="编程语言学习总结">编程语言学习总结</a><time datetime="2024-01-04T04:57:53.000Z" title="发表于 2024-01-04 12:57:53">2024-01-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/01/03/Transformer%E5%8A%A0%E9%80%9F%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/" title="Transformer加速器学习总结">Transformer加速器学习总结</a><time datetime="2024-01-03T13:45:48.000Z" title="发表于 2024-01-03 21:45:48">2024-01-03</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/top.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By kzw3933</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>